# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HLn6jWqQVymWeo4kJsUpVI3JA4S9DgiE

<h1 align="center">AI Laundry Sorter</h1>
<h3 align="center">Sepideh Forouzi</h3>

# Introduction

In this project, I developed an intelligent, end-to-end laundry-sorting system using a large-scale real-world fashion dataset. The dataset I used originates from the **H&M Personalized Fashion Recommendations dataset** on Kaggle (https://www.kaggle.com/code/gpreda/h-m-eda-and-prediction/input). It contains **over 105,000 clothing images** accompanied by rich metadata such as color attributes, garment types, fabric descriptions, washing instructions, and article identifiers. From this extensive dataset, I curated and refined a high-quality subset of approximately **16,800 labeled samples** specifically tailored for my multi-task deep learning pipeline.

The primary goal of my project is to design an automated system that can **analyze a single image of a garment and recommend the correct washing program**, including color grouping, fabric identification, and the appropriate wash cycle. To achieve this, I trained a **four-head multitask ConvNeXt-based model** capable of simultaneously predicting:

1. **Color group**  
2. **Fabric group**  
3. **Washing-cycle category**  
4. **Whether the image actually contains clothing (binary IS_CLOTHING gate)**  

By framing the problem as a multitask learning setup, I leveraged shared visual representations that contribute jointly to color, fabric, and wash-instruction recognition, significantly improving the model’s stability and efficiency. The architecture builds on **ConvNeXt-Tiny**, a modern convolutional backbone with strong generalization performance and highly stable gradient behavior—ideal for dense multi-head classification tasks.

Throughout the preprocessing and feature-engineering pipeline, I ensured that **all positional and spatial features remained strictly non-negative**. This prevents inconsistencies in augmentation, bounding-box handling, and feature extraction, and avoids numerical artifacts that could negatively affect the downstream ConvNeXt feature maps.

Overall, the project demonstrates how deep learning can automate a highly practical household task—**selecting the correct washing program using only an image of the garment**. The trained multitask model forms the core of a fully functional **Streamlit-based demo**, providing real-time color, fabric, and washing-cycle predictions through a simple and intuitive image-upload interface.

---

### 1) Preprocessing

In this stage, **I prepared the full data layer**, including metadata cleaning, image verification, and the initialization of all libraries required for the downstream deep-learning pipeline.

#### • Library Initialization
I began by importing fundamental Python modules such as `os`, `pathlib`, `re`, and `math` to handle filesystem navigation, pattern matching, and various utility operations. I used `tqdm` to wrap long-running loops with progress bars for better monitoring.

For data manipulation, I loaded `numpy` and `pandas`, which I relied on throughout the project for structured processing, type coercion, categorical cleanup, and generating the metadata tables required for model training.

For image handling, I used `PIL.Image` to open, normalize, and verify images in a consistent RGB format. This gave me a reliable mechanism to detect unreadable or corrupted files.

On the deep-learning side, I initialized the PyTorch stack:  
- `torch` for tensor operations,  
- `torch.nn` for network layers,  
- `Dataset` and `DataLoader` for building a custom image-metadata input pipeline, and  
- `torchvision`/`torchvision.transforms` for applying standardized augmentations, resizing, normalization, and image-to-tensor conversion.

I also attempted to import the `timm` library, which contains modern backbones such as ConvNeXt and EfficientNet. I wrapped this import in a `try/except` block so that when `timm` is unavailable, the code automatically and safely falls back to `torchvision` architectures. This ensured reproducibility and prevented environment errors.

---

### Metadata Cleaning (`articles.csv`)

I performed an extensive cleaning pass on the raw H&M metadata:

- Normalized all column names (lowercase, snake-case, stripped whitespace).  
- Applied **Unicode NFKC** normalization and performed **HTML-tag removal** for free-text columns like `detail_desc`.  
- Coerced numeric fields (`article_id`, `product_code`, `department_no`, etc.) and converted all `*date*` columns using safe datetime parsing.  
- Removed rows with missing essential keys (`article_id`, `prod_name`) and eliminated exact and near-duplicate entries.  
- Standardized categorical fields through case-folding, whitespace normalization, and a synonym-mapping step.  
- Validated numeric domains and replaced invalid codes with `NaN`.  
- Exported a fully cleaned table to `processed/articles_clean.csv`, along with a column dictionary and a data-quality summary.

---

### Image Verification

I scanned all images inside `crops/train` and `crops/val` recursively.  
For each file, I attempted `PIL.Image.verify()` to detect unreadable or corrupted images.

I then exported:

- `image_inventory.csv` — full listing  
- `image_inventory_ok.csv` — readable images  
- `image_inventory_bad.csv` — unreadable or corrupted files  

This allowed me to confirm dataset integrity before constructing the training DataLoader.

---

### Result
After preprocessing, **I obtained a clean, validated metadata table and a verified image inventory**, ensuring that the rest of the training pipeline operates on consistent, high-quality inputs.
"""

# Cell 1a: Core imports (no installation commands)

import os
import re
import math
import colorsys

import numpy as np
import pandas as pd
from PIL import Image
from tqdm.auto import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import torchvision

# timm backbone (ConvNeXt / EfficientNet)
try:
    import timm
    print("timm imported successfully.")
except ImportError:
    timm = None
    print("timm is NOT available — will fall back to torchvision backbones if needed.")

# ============================================================
#  Cell 1b-Data Preprocessing (articles.csv + image inventory) — EXTENDED CLEAN
# ============================================================
from pathlib import Path
from collections import Counter
import unicodedata
import re

# ---------- helpers ----------
def _unicode_nfkc(s: object) -> object:
    if isinstance(s, str):
        return unicodedata.normalize("NFKC", s)
    return s

def _strip_html(s: object) -> object:
    if not isinstance(s, str):
        return s
    # remove tags and collapse whitespace
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s if s else None

def _strip_norm_strings(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # normalize column names
    df.columns = (
        df.columns
        .str.strip()
        .str.replace(r"\s+", "_", regex=True)
        .str.replace(r"[^0-9a-zA-Z_]", "", regex=True)
        .str.lower()
    )
    # normalize cell strings
    for c in df.columns:
        if pd.api.types.is_object_dtype(df[c]):
            df[c] = (
                df[c]
                .map(_unicode_nfkc)
                .astype("string")
                .str.replace(r"\s+", " ", regex=True)
                .str.strip()
            )
            # empty → <NA>
            df[c] = df[c].replace({"": pd.NA})
    return df

def _missing_report(df: pd.DataFrame) -> pd.DataFrame:
    miss = df.isna().sum()
    pct = (miss / len(df) * 100).round(2)
    rep = pd.DataFrame({"missing": miss, "percent": pct})
    return rep[rep["missing"] > 0].sort_values("percent", ascending=False)

def _coerce_int(series, errors="coerce"):
    return pd.to_numeric(series, errors=errors, downcast="integer")

def _coerce_float(series, errors="coerce"):
    return pd.to_numeric(series, errors=errors, downcast="float")

def _safe_read_image(p: Path) -> bool:
    try:
        with Image.open(p) as im:
            im.verify()
        return True
    except Exception:
        return False

# ---------- ensure processed/ ----------
PROCESSED_DIR = os.path.join(WASH_ROOT, "processed")
os.makedirs(PROCESSED_DIR, exist_ok=True)

# ---------- 1) load & clean CSV ----------
if not os.path.exists(ARTICLES_CSV):
    raise FileNotFoundError(f"ARTICLES_CSV not found at {ARTICLES_CSV}")

print("\n[Preprocess] Loading articles.csv ...")
articles_raw = pd.read_csv(
    ARTICLES_CSV,
    dtype="unicode",
    keep_default_na=True,
    na_values=["", "NA", "NaN", "None", "null"]
)
print(f"[Preprocess] Loaded {len(articles_raw):,} rows.")

# normalize strings and columns
articles = _strip_norm_strings(articles_raw)

# HTML strip & unicode normalize for description-like fields
for dc in ["detail_desc", "product_display_name", "prod_name"]:
    if dc in articles.columns:
        articles[dc] = articles[dc].map(_strip_html).map(_unicode_nfkc).astype("string")

# type coercions (if exist)
for c in ["article_id", "product_code", "department_no", "product_type_no", "section_no"]:
    if c in articles.columns:
        articles[c] = _coerce_int(articles[c])

# parse date-like columns
for dc in [c for c in articles.columns if "date" in c]:
    articles[dc] = pd.to_datetime(articles[dc], errors="coerce")

print("\n[Preprocess] Missingness BEFORE cleaning:")
rep_before = _missing_report(articles)
print(rep_before if not rep_before.empty else "(no missing values)")

# essential keys & dedup
essential = []
if "article_id" in articles.columns: essential.append("article_id")
if "prod_name"  in articles.columns: essential.append("prod_name")

before_drop = len(articles)
for key in essential:
    articles = articles[~articles[key].isna()]

# remove exact duplicates
articles = articles.drop_duplicates()
# prefer unique article_id if present
if "article_id" in articles.columns:
    articles = articles.drop_duplicates(subset=["article_id"])

after_drop = len(articles)
if after_drop != before_drop:
    print(f"[Preprocess] Dropped {before_drop - after_drop:,} rows (NaNs/dupes).")

# categorical standardization
cat_candidates = [
    "product_type_name","product_group_name","graphical_appearance_name",
    "colour_group_name","perceived_colour_value_name","perceived_colour_master_name",
    "department_name","index_name","index_group_name","section_name","garment_group_name"
]
for c in cat_candidates:
    if c in articles.columns:
        s = articles[c].astype("string")
        # casefold → title-case; keep Unknown for NAs
        s = s.fillna("Unknown")
        s_norm = (
            s.str.casefold()
             .str.replace(r"\s+", " ", regex=True)
             .str.strip()
        )
        # simple synonym map (extend as needed)
        syn = {
            "tshirt":"T-shirt", "t shirts":"T-shirt", "t-shirt":"T-shirt",
            "colour blocking":"Colour Blocking", "color blocking":"Colour Blocking",
            "mixed solid/pattern":"Mixed Solid/Pattern", "front print":"Front Print",
            "placement print":"Placement Print", "all over pattern":"All Over Pattern",
        }
        s_norm = s_norm.map(lambda x: syn.get(x, x))
        # title-case except tokens like “T-shirt”
        s_final = s_norm.apply(lambda x: x if x in syn.values() else x.title())
        articles[c] = s_final.replace({"Unknown":"Unknown"}).astype("string")

# numeric domain sanity checks (coerce invalid → NaN)
if "department_no" in articles.columns:
    # typical H&M dept in [0, 999]; relax as needed
    mask = (articles["department_no"] < 0) | (articles["department_no"] > 999)
    articles.loc[mask, "department_no"] = pd.NA
if "section_no" in articles.columns:
    mask = (articles["section_no"] < 0) | (articles["section_no"] > 9999)
    articles.loc[mask, "section_no"] = pd.NA
if "product_code" in articles.columns:
    mask = (articles["product_code"] <= 0)
    articles.loc[mask, "product_code"] = pd.NA

# ensure article_id is integer-like where possible AND keep a string view for joins
if "article_id" in articles.columns:
    articles["article_id_int"] = _coerce_int(articles["article_id"])
    articles["article_id"] = articles["article_id"].astype("string")

print("\n[Preprocess] Missingness AFTER cleaning:")
rep_after = _missing_report(articles)
print(rep_after if not rep_after.empty else "(no missing values)")

nunq = articles.nunique(dropna=False).sort_values(ascending=False)
print("\n[Preprocess] Cardinalities (top 10):")
print(nunq.head(10))

ARTICLES_CLEAN = os.path.join(PROCESSED_DIR, "articles_clean.csv")
articles.to_csv(ARTICLES_CLEAN, index=False)
print(f"\n[Preprocess] Wrote cleaned articles → {ARTICLES_CLEAN}")

# ---------- 2) quick image sweep under crops/ ----------
if os.path.exists(CROPS_DIR):
    print("\n[Preprocess] Scanning crops/ for unreadable images ...")
    rows = []
    for split in ["train","val"]:
        sp = Path(CROPS_DIR) / split
        if not sp.exists(): continue
        files = list(sp.rglob("*.jpg")) + list(sp.rglob("*.jpeg")) + list(sp.rglob("*.png"))
        for p in tqdm(files, desc=f"[Scan] {split}"):
            rows.append({"split":split, "path":str(p), "ok":_safe_read_image(p)})
    inv = pd.DataFrame(rows)
    if inv.empty:
        print("[Preprocess] No images found under crops/.")
    else:
        bad = inv[~inv["ok"]]; good = inv[inv["ok"]]
        inv.to_csv(os.path.join(PROCESSED_DIR, "image_inventory.csv"), index=False)
        good.to_csv(os.path.join(PROCESSED_DIR, "image_inventory_ok.csv"), index=False)
        bad.to_csv(os.path.join(PROCESSED_DIR, "image_inventory_bad.csv"), index=False)
        print(f"[Preprocess] Total: {len(inv):,} | OK: {len(good):,} | BAD: {len(bad):,}")
else:
    print(f"\n[Preprocess] CROPS_DIR not found at {CROPS_DIR} — skipping image scan.")

# ---------- 3) column dictionary + compact DQ summary ----------
col_source = pd.read_csv(ARTICLES_CLEAN, dtype="unicode")
col_dict = pd.DataFrame({
    "column": col_source.columns,
    "dtype": [str(col_source[c].dtype) for c in col_source.columns],
    "nunique": [col_source[c].nunique(dropna=False) for c in col_source.columns],
    "missing_pct": [(col_source[c].isna().mean()*100).round(2) for c in col_source.columns],
    "example": [col_source[c].dropna().iloc[0] if col_source[c].notna().any() else None for c in col_source.columns],
}).sort_values("missing_pct", ascending=False)
col_dict_path = os.path.join(PROCESSED_DIR, "articles_column_dictionary.csv")
col_dict.to_csv(col_dict_path, index=False)

dq_path = os.path.join(PROCESSED_DIR, "data_quality_summary.txt")
with open(dq_path, "w", encoding="utf-8") as f:
    f.write("=== Data Quality Summary ===\n")
    f.write(f"Rows after cleaning: {len(articles):,}\n")
    f.write("Top missing columns:\n")
    f.write(rep_after.head(10).to_string() if not rep_after.empty else "(none)")
    f.write("\n\nTop cardinalities:\n")
    f.write(nunq.head(10).to_string())

print(f"\n[Preprocess] Column dictionary → {col_dict_path}")
print(f"[Preprocess] Data quality summary → {dq_path}")

# ============================================================
# Cell 1c — Enforce No-Missing Policy (after preprocessing)
# Produces: processed/articles_clean_nomissing.csv
# ============================================================
from pathlib import Path

ARTICLES_CLEAN = os.path.join(WASH_ROOT, "processed", "articles_clean.csv")
OUT_NO_MISS    = os.path.join(WASH_ROOT, "processed", "articles_clean_nomissing.csv")

if not os.path.exists(ARTICLES_CLEAN):
    raise FileNotFoundError(f"Cleaned file not found: {ARTICLES_CLEAN} — run the preprocessing cell first.")

# 1) Load cleaned table
df = pd.read_csv(ARTICLES_CLEAN, low_memory=False)

# Detect datetime-like columns by name and parse them
date_cols = [c for c in df.columns if "date" in c.lower()]
for c in date_cols:
    try:
        df[c] = pd.to_datetime(df[c], errors="coerce")
    except Exception:
        # if parsing fails, leave as-is (string) and handle below
        pass

# 2) Report current missingness
miss_before = df.isna().sum().sum()
print(f"[NoMissing] Missing cells BEFORE fill: {miss_before:,}")

# 3) Column-wise policies
#    - IDs/keys: must not be missing → drop rows lacking essential keys
ESSENTIAL_KEYS = [c for c in ["article_id", "prod_name"] if c in df.columns]
for key in ESSENTIAL_KEYS:
    n_drop = df[key].isna().sum()
    if n_drop > 0:
        print(f"[NoMissing] Dropping {n_drop} rows missing essential key: {key}")
        df = df[~df[key].isna()].copy()

#    - Numeric columns: fill NaN with median (fallback to 0 if all-NaN)
num_cols = df.select_dtypes(include=["number"]).columns.tolist()
for c in num_cols:
    if df[c].isna().any():
        if df[c].notna().any():
            med = df[c].median()
            df[c] = df[c].fillna(med)
        else:
            df[c] = df[c].fillna(0)

#    - Datetime columns (parsed above): fill NaT with epoch
for c in date_cols:
    if pd.api.types.is_datetime64_any_dtype(df[c]):
        if df[c].isna().any():
            df[c] = df[c].fillna(pd.Timestamp("1970-01-01"))

#    - Non-numeric, non-datetime (object/string): fill with "Unknown"
non_num_cols = df.columns.difference(num_cols)
for c in non_num_cols:
    if pd.api.types.is_datetime64_any_dtype(df[c]):
        # already handled in datetime block
        continue
    # treat as textual; also normalize empty strings → Unknown
    if df[c].dtype == "O" or pd.api.types.is_string_dtype(df[c]):
        df[c] = df[c].astype("string")
        df[c] = df[c].replace({"": pd.NA})
        if df[c].isna().any():
            df[c] = df[c].fillna("Unknown")

# 4) Final assert: absolutely no missing values remain
miss_after = df.isna().sum().sum()
print(f"[NoMissing] Missing cells AFTER  fill: {miss_after:,}")
assert miss_after == 0, "There are still missing values after filling — inspect the log above."

# 5) Enforce strict dtypes on common columns
#    (keeps article_id as string for joins; ensures product_code integer-like)
if "article_id" in df.columns:
    df["article_id"] = df["article_id"].astype("string")

for c in ["product_code", "department_no", "product_type_no", "section_no"]:
    if c in df.columns:
        # cast to integer if it is integral after filling
        try:
            # if values are float but integral, cast down; otherwise keep as numeric
            if pd.api.types.is_float_dtype(df[c]) and (df[c] % 1 == 0).all():
                df[c] = df[c].astype("Int64")
        except Exception:
            pass

# 6) Save artifact with no missing values
df.to_csv(OUT_NO_MISS, index=False)
print(f"[NoMissing] Wrote → {OUT_NO_MISS}")

"""---

### 2) Google Drive Mounting & Project Path Resolution

In this step, I mounted my Google Drive to the Colab environment and resolved all directory paths required for the project. Since the dataset, the cropped garment images, and the model outputs were all stored inside my Drive, establishing a reliable and consistent path structure was essential before proceeding with any preprocessing or model training.

I first mounted Google Drive and inspected the top-level directory to confirm that all relevant folders—such as `wash_ai_project`, `hm`, and the crop directories—were present. Because different machines or uploads sometimes introduce slight variations in folder structure, I checked multiple candidate paths for the project root (`wash_ai_project`). I resolved the correct path dynamically by verifying which candidate actually existed.

After identifying the correct project root, I constructed internal paths such as the `crops/` directory, which contains the training and validation images used for classification. I then performed the same detection logic for the H&M dataset, locating the folder that contains the official `articles.csv` metadata file. This metadata file plays a central role in linking product IDs to garment attributes such as product type, color, index name, and season information.

Finally, I performed an existence check on all critical paths—`CROPS_DIR`, training and validation subfolders, and the H&M metadata file—ensuring that the entire project structure was valid and ready for downstream processing. This path-resolution step eliminated any ambiguity and guaranteed full reproducibility of the pipeline, regardless of the environment in which the notebook was executed.

"""

# ============================================================
# Cell 2 — Mount Google Drive & Resolve Project Paths
# ============================================================

from google.colab import drive
drive.mount('/content/drive')

print("\n=== MyDrive — Top-level contents ===")
print(os.listdir("/content/drive/MyDrive"))

# ------------------------------------------------------------
# Detect wash_ai_project root
# ------------------------------------------------------------
candidate1 = "/content/drive/MyDrive/content/wash_ai_project"
candidate2 = "/content/drive/MyDrive/wash_ai_project"

print("\nChecking candidate WASH_ROOT paths:")
print("Candidate 1 exists? ", os.path.exists(candidate1))
print("Candidate 2 exists? ", os.path.exists(candidate2))

WASH_ROOT = candidate1 if os.path.exists(candidate1) else candidate2
CROPS_DIR = os.path.join(WASH_ROOT, "crops")

# ------------------------------------------------------------
# Detect HM Articles dataset root
# ------------------------------------------------------------
hm1 = "/content/drive/MyDrive/hm"
hm2 = "/content/drive/MyDrive/HM"

print("\nChecking candidate HM_ROOT paths:")
print("hm1 exists? ", os.path.exists(hm1))
print("hm2 exists? ", os.path.exists(hm2))

HM_ROOT = hm1 if os.path.exists(hm1) else hm2
ARTICLES_CSV = os.path.join(HM_ROOT, "articles.csv")

print("\n=== Final Selected Paths ===")
print("WASH_ROOT:    ", WASH_ROOT)
print("CROPS_DIR:    ", CROPS_DIR)
print("HM_ROOT:      ", HM_ROOT)
print("ARTICLES_CSV: ", ARTICLES_CSV)

print("\n=== Existence Check ===")
print("CROPS_DIR exists:      ", os.path.exists(CROPS_DIR))
print("Train folder exists:   ", os.path.exists(os.path.join(CROPS_DIR, "train")))
print("Val folder exists:     ", os.path.exists(os.path.join(CROPS_DIR, "val")))
print("ARTICLES_CSV exists:   ", os.path.exists(ARTICLES_CSV))

"""---
### 3.)Articles Metadata Cleaning: Garments vs Accessories & Fabric Extraction

In the next preprocessing step, I focused on cleaning and structuring the `articles.csv` file provided by the H&M dataset. This file contains high-level product metadata (e.g., product type, group, color, and detailed description), which I used to define semantic garment attributes for the downstream deep learning tasks.

First, I loaded `articles.csv` with `article_id` kept as a string to avoid any loss of leading zeros in the identifiers. The raw file contained **105,542** rows and the expected columns such as `product_type_name`, `graphical_appearance_name`, and `detail_desc`, which were crucial for my pipeline.

I then separated **garments** from **accessories** using the `product_type_name` field. To do this, I defined a set of product types corresponding to accessories (e.g., *boots, shoes, sandals, bags, wallets, belts, scarves, hats, jewelry, phone cases*, etc.) and treated all remaining items as garments. After this filtering:

- Total rows in `articles.csv`: **105,542**  
- Garment rows used for clothing-related tasks: **95,712**  
- Accessory rows stored as **NON-CLOTHING** examples for the 4th head: **9,830**

The accessory subset was not discarded; instead, I kept it explicitly to later train the **IS_CLOTHING** head (clothing vs non-clothing) as part of the 4-head multitask model.

Next, I extracted a **high-level fabric category** `fabric_group` from the free-text `detail_desc` field for garments only. I parsed the description in a rule-based, priority-driven manner:

- If the description contained “cotton” → `COTTON`
- Else if it contained “linen” or “flax” → `LINEN`
- Else if it contained “wool” or “merino` → `WOOL`
- Else if it contained “silk” → `SILK`
- Else if it contained any synthetic-related keyword  
  (e.g. *polyester, nylon, polyamide, acrylic, viscose, modal, lyocell, rayon, elastane, spandex*) → `SYNTHETIC`
- Otherwise → `UNKNOWN`

To preserve label quality, I removed all garments with `fabric_group = UNKNOWN`. This step eliminated **50,502** rows, leaving **45,210** garments with a well-defined fabric category:

- Garments before fabric filtering: **95,712**  
- Garments removed as `UNKNOWN` fabric: **50,502**  
- Garments used for training COLOR/FABRIC/WASH heads: **45,210**

I then defined a binary **pattern flag** `pattern_flag` from the `graphical_appearance_name` column. I mapped specific names such as *"All over pattern"*, *"Stripe"*, *"Check"*, *"Front print"*, *"Placement print"*, *"Colour blocking"*, *"Dot"*, *"Mixed solid/pattern"*, and *"Other pattern"* to `pattern_flag = 1`, and all other appearances to `pattern_flag = 0`. This provided a compact indicator of whether a garment had an explicit graphical or patterned design, which can interact with both color perception and washing recommendations.

Finally, I set `article_id` as the index for both the cleaned **garment table** and the **accessory table**, enabling fast and consistent lookups throughout the rest of the pipeline. At the end of this step, I obtained:

- A cleaned **GARMENTS** metadata table (45,210 rows) used for:
  - **COLOR** head (color groups)
  - **FABRIC** head (fabric groups)
  - **WASH_CYCLE** head (washing programs)
- An **ACCESSORIES** metadata table (9,830 rows) reserved for:
  - **IS_CLOTHING** head (NON-CLOTHING examples)

This structured metadata became the backbone for linking each cropped image to meaningful, high-level semantic labels in my multitask deep learning model.

"""

# ============================================================
# Cell 3 — Load articles.csv, separate ACCESSORIES, build garment metadata
# ============================================================

# 3.0 Load articles.csv (keep article_id as string)
articles = pd.read_csv(
    ARTICLES_CSV,
    dtype={"article_id": str},
    low_memory=False,
)

print("Columns in articles.csv:")
print(articles.columns.tolist())

# ------------------------------------------------------------
# 3.1 Separate ACCESSORIES vs GARMENTS using product_type_name
# ------------------------------------------------------------
ACCESSORY_TYPES = {
    # Extend this set if you see more accessory names in your file
    "boots", "shoes", "sneakers", "sandals", "flip-flops", "heels",
    "bag", "backpack", "wallet", "purse", "clutch",
    "belt", "scarf", "shawl", "sunglasses",
    "necklace", "bracelet", "earrings", "ring", "hair/alice band",
    "cap/peaked", "hat", "gloves",
    "tie", "bow tie", "phone case", "tablet case",
}

ptype_col = "product_type_name"
if ptype_col not in articles.columns:
    raise KeyError(
        f"Expected column '{ptype_col}' not found in articles.csv. "
        f"Available columns: {articles.columns.tolist()}"
    )

before_total = len(articles)

# Normalize product_type_name to lowercase for robust matching
articles[ptype_col] = articles[ptype_col].astype(str)
ptype_lower = articles[ptype_col].str.lower()

mask_accessory = ptype_lower.isin(ACCESSORY_TYPES)
mask_garment   = ~mask_accessory

# accessories → will be used later as NON-CLOTHING examples for the 4th head
articles_accessory = articles[mask_accessory].copy()

# garments → used for COLOR / FABRIC / WASH tasks
articles = articles[mask_garment].copy()

print(f"\nTotal rows in articles.csv       : {before_total}")
print(f"Garment rows (for clothing tasks): {len(articles)}")
print(f"Accessory rows (for NON-CLOTHING): {len(articles_accessory)}")

# ------------------------------------------------------------
# 3.2 Extract fabric_group from detail_desc (GARMENTS ONLY)
# ------------------------------------------------------------
if "detail_desc" not in articles.columns:
    raise KeyError(
        "Expected 'detail_desc' column for fabric parsing "
        f"but not found. Available columns: {articles.columns.tolist()}"
    )

def extract_fabric_group(detail_desc: str) -> str:
    if not isinstance(detail_desc, str):
        return "UNKNOWN"
    text = detail_desc.lower()

    # Priority: cotton > linen > wool > silk > synthetic
    if "cotton" in text:
        return "COTTON"
    if "linen" in text or "flax" in text:
        return "LINEN"
    if "wool" in text or "merino" in text:
        return "WOOL"
    if "silk" in text:
        return "SILK"

    synthetic_keywords = [
        "polyester", "polyamide", "nylon", "acrylic",
        "viscose", "modal", "lyocell", "rayon",
        "elastane", "spandex",
    ]
    if any(kw in text for kw in synthetic_keywords):
        return "SYNTHETIC"

    return "UNKNOWN"

articles["fabric_group"] = articles["detail_desc"].apply(extract_fabric_group)

before_fabric = len(articles)
articles = articles[articles["fabric_group"] != "UNKNOWN"].copy()
after_fabric = len(articles)

print(
    f"\nRemoved {before_fabric - after_fabric} garment rows with "
    f"UNKNOWN fabric_group; remaining garments: {after_fabric}"
)

# ------------------------------------------------------------
# 3.3 Define pattern_flag from graphical_appearance_name (if available)
# ------------------------------------------------------------
pattern_col = "graphical_appearance_name"
if pattern_col in articles.columns:
    PATTERN_NAMES = {
        "All over pattern",
        "Stripe",
        "Check",
        "Front print",
        "Placement print",
        "Colour blocking",
        "Dot",
        "Mixed solid/pattern",
        "Other pattern",
    }
    articles["pattern_flag"] = articles[pattern_col].isin(PATTERN_NAMES).astype(int)
else:
    print(
        f"\nWarning: '{pattern_col}' not found in articles.csv. "
        "Setting pattern_flag = 0 for all garment rows."
    )
    articles["pattern_flag"] = 0

# ------------------------------------------------------------
# 3.4 Set index by article_id for fast lookup (garments + accessories)
# ------------------------------------------------------------
if "article_id" not in articles.columns:
    raise KeyError("Expected 'article_id' column not found in articles.csv.")

articles = articles.set_index("article_id", drop=False)

if "article_id" in articles_accessory.columns:
    articles_accessory = articles_accessory.set_index("article_id", drop=False)

print("\nSample of cleaned GARMENT metadata:")
print(articles[[ptype_col, "fabric_group", "pattern_flag"]].head())

print("\nFinal counts:")
print("Garments after cleaning (used for 3 heads)   :", len(articles))
print("Accessories stored (used for 4th NON-CLOTHING head):", len(articles_accessory))

"""---
### 4)Drive Mounting and Path Configuration

In this step, I mounted my Google Drive in Colab and inspected the top-level folders under `MyDrive`. I then automatically resolved all relevant project paths:

- I detected the project root `WASH_ROOT` as  
  `/content/drive/MyDrive/wash_ai_project`.
- I defined the cropped-image root `CROPS_DIR` as  
  `/content/drive/MyDrive/wash_ai_project/crops`.
- I located the H&M metadata root `HM_ROOT` as  
  `/content/drive/MyDrive/hm` and the main metadata file  
  `ARTICLES_CSV = /content/drive/MyDrive/hm/articles.csv`.
- I also specified the expected training and validation folders  
  `TRAIN_DIR = CROPS_DIR/train` and `VAL_DIR = CROPS_DIR/val`.

Finally, I performed an existence check on all these paths. At this stage, `CROPS_DIR` and `ARTICLES_CSV` existed, while the `train` and `val` subfolders under `crops` had not yet been created, which I later addressed in the subsequent preprocessing steps.

"""

# Cell 4 — Mount Google Drive and configure all project paths

import os
from google.colab import drive

# 1. Mount Google Drive
drive.mount('/content/drive')

# 2. Inspect top-level folders
print("\n=== Top-level folders in MyDrive ===")
print(os.listdir("/content/drive/MyDrive"))

# 3. Candidate WASH_ROOT paths
possible_wash_roots = [
    "/content/drive/MyDrive/content/wash_ai_project",
    "/content/drive/MyDrive/wash_ai_project"
]

WASH_ROOT = next((p for p in possible_wash_roots if os.path.exists(p)), None)
if WASH_ROOT is None:
    raise FileNotFoundError("Could not find wash_ai_project folder in MyDrive.")

CROPS_DIR = os.path.join(WASH_ROOT, "crops")

# 4. Candidate H&M paths
possible_hm_roots = [
    "/content/drive/MyDrive/hm",
    "/content/drive/MyDrive/HM"
]

HM_ROOT = next((p for p in possible_hm_roots if os.path.exists(p)), None)
if HM_ROOT is None:
    raise FileNotFoundError("Could not find HM folder in MyDrive.")

ARTICLES_CSV = os.path.join(HM_ROOT, "articles.csv")

# 5. TRAIN / VAL paths
TRAIN_DIR = os.path.join(CROPS_DIR, "train")
VAL_DIR   = os.path.join(CROPS_DIR, "val")

# 6. Print summary
print("\n=== Final Selected Paths ===")
print("WASH_ROOT   :", WASH_ROOT)
print("CROPS_DIR    :", CROPS_DIR)
print("HM_ROOT      :", HM_ROOT)
print("ARTICLES_CSV :", ARTICLES_CSV)
print("TRAIN_DIR    :", TRAIN_DIR)
print("VAL_DIR      :", VAL_DIR)

print("\n=== Existence Check ===")
print("CROPS_DIR exists:", os.path.exists(CROPS_DIR))
print("Train folder exists:", os.path.exists(TRAIN_DIR))
print("Val folder exists:", os.path.exists(VAL_DIR))
print("ARTICLES_CSV exists:", os.path.exists(ARTICLES_CSV))

"""---
##  EDA part - Data Analysis
---

### 5) Building the Label Table (`wash_labels.csv`)

In this step, I constructed the master label table that links every cropped image to its semantic targets. I first defined all core paths to the project root, the train/validation crop folders, and the original `articles.csv` metadata file from the H&M dataset. Then I loaded `articles.csv`, separated garments from accessories based on `product_type_name`, and retained only garments with a valid `fabric_group` inferred from the free-text `detail_desc`. I also derived a binary `pattern_flag` from `graphical_appearance_name` to indicate whether the item had a visible pattern.

Next, I scanned all images in the `train` and `val` crop folders, extracted the 10-digit `article_id` from each filename, and joined them with the cleaned garment/accessory metadata. For garments, I computed mean HSV values from the image pixels and used them together with `pattern_flag` to assign a discrete `color_group` (LIGHT, DARK, COLOR, COLORFUL). I encoded both `color_group` and `fabric_group` into integer labels and set `is_clothing = 1`. For accessories, I treated them as `NON_CLOTHING` (`is_clothing = 0`) and optionally stored their HSV statistics without color/fabric labels.

Finally, I assembled all rows into a single DataFrame, added an explicit binary `clothing_label` (mirroring `is_clothing`) for the fourth head of my multitask network, and saved the result as `wash_labels.csv`. This file became the central supervision source for color, fabric, clothing vs. non-clothing, and later the wash-cycle prediction task.
"""

# ============================================================
# MASTER CELL — Build wash_labels.csv
#  - Clothing images: color + fabric labels + is_clothing = 1
#  - Accessory images: is_clothing = 0 (non-clothing class)
#  - NEW: clothing_label column for 4th head (0/1)
# ============================================================

import os
from pathlib import Path
import re
import numpy as np
import pandas as pd
from PIL import Image
import colorsys
from tqdm.auto import tqdm

# ------------------------------------------------------------
# 0) Core paths
# ------------------------------------------------------------

WASH_ROOT    = "/content/drive/MyDrive/wash_ai_project"
TRAIN_DIR    = "/content/drive/MyDrive/wash_ai_outputs/crops_backup/content/wash_ai_project/crops/train"
VAL_DIR      = "/content/drive/MyDrive/wash_ai_outputs/crops_backup/content/wash_ai_project/crops/val"
ARTICLES_CSV = "/content/drive/MyDrive/hm/articles.csv"

LABELS_CSV = os.path.join(WASH_ROOT, "wash_labels.csv")

print("WASH_ROOT   :", WASH_ROOT)
print("TRAIN_DIR   :", TRAIN_DIR)
print("VAL_DIR     :", VAL_DIR)
print("ARTICLES_CSV:", ARTICLES_CSV)

assert os.path.exists(TRAIN_DIR),     f"TRAIN_DIR not found: {TRAIN_DIR}"
assert os.path.exists(VAL_DIR),       f"VAL_DIR not found: {VAL_DIR}"
assert os.path.exists(ARTICLES_CSV),  f"articles.csv not found: {ARTICLES_CSV}"

# ------------------------------------------------------------
# 1) Load and clean articles.csv (metadata)
# ------------------------------------------------------------

print("\n[1] Loading articles.csv ...")
articles = pd.read_csv(
    ARTICLES_CSV,
    dtype={"article_id": str},
    low_memory=False
)

print("Columns in articles.csv:", list(articles.columns))

# 1.1 Separate garments vs. accessories
ACCESSORY_TYPES = {
    "Boots", "Shoes", "Sneakers", "Sandals", "Flip-flops", "Heels",
    "Bag", "Backpack", "Wallet", "Purse", "Clutch",
    "Belt", "Scarf", "Shawl", "Sunglasses",
    "Necklace", "Bracelet", "Earrings", "Ring", "Hair/alice band",
    "Cap/peaked", "Hat", "Gloves",
    "Tie", "Bow tie", "Phone case", "Tablet case"
}

ptype_col = "product_type_name"
if ptype_col not in articles.columns:
    raise KeyError(f"Expected column '{ptype_col}' not found in articles.csv")

before = len(articles)

mask_accessory      = articles[ptype_col].isin(ACCESSORY_TYPES)
articles_accessory  = articles[mask_accessory].copy()       # NON-CLOTHING metadata
articles_garments   = articles[~mask_accessory].copy()      # CLOTHING metadata

print(f"\nTotal rows in articles.csv: {before}")
print(f"Garment rows:   {len(articles_garments)}")
print(f"Accessory rows: {len(articles_accessory)}")

# 1.2 Fabric group from detail_desc (for garments only)
if "detail_desc" not in articles_garments.columns:
    raise KeyError("Expected 'detail_desc' column not found in articles_garments")

def extract_fabric_group(detail_desc: str) -> str:
    """
    Map free-text detail_desc to a coarse fabric group.
    """
    if not isinstance(detail_desc, str):
        return "UNKNOWN"
    t = detail_desc.lower()

    if "cotton" in t:
        return "COTTON"
    if "linen" in t or "flax" in t:
        return "LINEN"
    if "wool" in t or "merino" in t:
        return "WOOL"
    if "silk" in t:
        return "SILK"

    synthetic_keywords = [
        "polyester", "polyamide", "nylon", "acrylic",
        "viscose", "modal", "lyocell", "rayon",
        "elastane", "spandex"
    ]
    if any(kw in t for kw in synthetic_keywords):
        return "SYNTHETIC"

    return "UNKNOWN"

articles_garments["fabric_group"] = articles_garments["detail_desc"].apply(extract_fabric_group)

before_fg = len(articles_garments)
articles_garments = articles_garments[articles_garments["fabric_group"] != "UNKNOWN"].copy()
after_fg = len(articles_garments)
print(f"\nRemoved {before_fg - after_fg} garment rows with UNKNOWN fabric_group; remaining garments: {after_fg}")

# 1.3 pattern_flag = 1 if graphical_appearance_name indicates a pattern
pattern_col = "graphical_appearance_name"
if pattern_col in articles_garments.columns:
    PATTERN_NAMES = {
        "All over pattern",
        "Stripe",
        "Check",
        "Front print",
        "Placement print",
        "Colour blocking",
        "Dot",
        "Mixed solid/pattern",
        "Other pattern"
    }
    articles_garments["pattern_flag"] = articles_garments[pattern_col].isin(PATTERN_NAMES).astype(int)
else:
    print(f"Warning: '{pattern_col}' not found. Setting pattern_flag = 0 for all garments.")
    articles_garments["pattern_flag"] = 0

# 1.4 Set index by article_id
if "article_id" not in articles_garments.columns:
    raise KeyError("Expected 'article_id' column not found in articles.csv")

articles_garments  = articles_garments.set_index("article_id", drop=False)
articles_accessory = articles_accessory.set_index("article_id", drop=False)

print("\nSample cleaned garment articles:")
print(articles_garments[[ptype_col, "fabric_group", "pattern_flag"]].head())

# ------------------------------------------------------------
# 2) Color helpers: HSV + rules
# ------------------------------------------------------------

def compute_hsv_mean(image_path: str, resize_to: int = 128):
    """
    Load an image, resize, convert to RGB, and compute mean H, S, V in [0, 1].
    """
    img = Image.open(image_path).convert("RGB")
    img = img.resize((resize_to, resize_to))
    arr = np.asarray(img, dtype=np.float32) / 255.0

    r = arr[..., 0].reshape(-1)
    g = arr[..., 1].reshape(-1)
    b = arr[..., 2].reshape(-1)

    hs, ss, vs = [], [], []
    for rr, gg, bb in zip(r, g, b):
        h, s, v = colorsys.rgb_to_hsv(float(rr), float(gg), float(bb))
        hs.append(h); ss.append(s); vs.append(v)

    return float(np.mean(hs)), float(np.mean(ss)), float(np.mean(vs))


def assign_color_group(pattern_flag: int, h: float, s: float, v: float) -> str:
    """
    Assign LIGHT / DARK / COLOR / COLORFUL based on HSV and pattern_flag.
    """
    if pattern_flag == 1:
        return "COLORFUL"

    if v <= 0.35:
        return "DARK"

    if v >= 0.70 and s <= 0.50:
        return "LIGHT"

    if s >= 0.60 and 0.35 < v < 0.85:
        return "COLORFUL"

    return "COLOR"


def extract_article_id_from_filename(filename: str) -> str | None:
    """
    Extract the first 10-digit sequence from the filename as article_id.
    Example: '0560208003_0.jpg' → '0560208003'
    """
    m = re.search(r"(\d{10})", filename)
    if m:
        return m.group(1)
    return None

COLOR2IDX    = {"LIGHT": 0, "DARK": 1, "COLORFUL": 2, "COLOR": 3}
FABRIC2IDX   = {"COTTON": 0, "LINEN": 1, "WOOL": 2, "SILK": 3, "SYNTHETIC": 4}
ISCLOTH2IDX  = {"NON_CLOTHING": 0, "CLOTHING": 1}

# ------------------------------------------------------------
# 3) Scan TRAIN / VAL images and build label rows
# ------------------------------------------------------------

def collect_all_images(root_dir: str):
    root = Path(root_dir)
    exts = {".jpg", ".jpeg", ".png", ".bmp"}
    paths = []
    for p in root.rglob("*"):
        if p.suffix.lower() in exts:
            paths.append(str(p))
    return paths

rows = []

splits = {
    "train": TRAIN_DIR,
    "val":   VAL_DIR,
}

total_seen       = 0
skipped_no_id    = 0
skipped_no_meta  = 0
skipped_error    = 0

print("\n[2] Building labels from TRAIN_DIR and VAL_DIR ...")
for split, root in splits.items():
    print(f"\nScanning split = '{split}' in: {root}")
    img_paths = collect_all_images(root)
    print(f"  Found {len(img_paths)} image files.")

    for fpath in tqdm(img_paths):
        total_seen += 1
        fname = os.path.basename(fpath)

        # 3.1 article_id from filename
        article_id = extract_article_id_from_filename(fname)
        if article_id is None:
            skipped_no_id += 1
            continue

        # 3.2 decide if this is a garment or an accessory
        if article_id in articles_garments.index:
            # Clothing sample
            meta = articles_garments.loc[article_id]
            pattern_flag = int(meta["pattern_flag"])
            fabric_group = meta["fabric_group"]

            if fabric_group not in FABRIC2IDX:
                skipped_error += 1
                continue

            # HSV + color_group
            try:
                h, s, v = compute_hsv_mean(fpath)
            except Exception:
                skipped_error += 1
                continue

            color_group = assign_color_group(pattern_flag, h, s, v)
            if color_group not in COLOR2IDX:
                skipped_error += 1
                continue

            color_label  = COLOR2IDX[color_group]
            fabric_label = FABRIC2IDX[fabric_group]
            is_clothing  = ISCLOTH2IDX["CLOTHING"]

            rows.append(
                {
                    "image_path": fpath,
                    "split": split,
                    "article_id": article_id,
                    "is_clothing": is_clothing,   # 1
                    "color_group": color_group,
                    "fabric_group": fabric_group,
                    "color_label": color_label,
                    "fabric_label": fabric_label,
                    "pattern_flag": pattern_flag,
                    "h": h, "s": s, "v": v,
                }
            )

        elif article_id in articles_accessory.index:
            # Accessory / NON-CLOTHING sample
            is_clothing = ISCLOTH2IDX["NON_CLOTHING"]  # 0

            # I can still compute HSV for analysis
            try:
                h, s, v = compute_hsv_mean(fpath)
            except Exception:
                skipped_error += 1
                continue

            rows.append(
                {
                    "image_path": fpath,
                    "split": split,
                    "article_id": article_id,
                    "is_clothing": is_clothing,
                    "color_group": None,
                    "fabric_group": None,
                    "color_label": np.nan,
                    "fabric_label": np.nan,
                    "pattern_flag": 0,
                    "h": h, "s": s, "v": v,
                }
            )
        else:
            # No metadata for this article_id at all
            skipped_no_meta += 1
            continue

df_labels = pd.DataFrame(rows)

print("\n=== Label building summary ===")
print("Total images seen:       ", total_seen)
print("Used images (rows):      ", len(df_labels))
print("Skipped (no article_id): ", skipped_no_id)
print("Skipped (no metadata):   ", skipped_no_meta)
print("Skipped (errors/other):  ", skipped_error)

print("\nSplit counts:")
print(df_labels["split"].value_counts())

print("\nHead of df_labels:")
print(df_labels.head())

# ------------------------------------------------------------
# 4) Add explicit clothing_label for 4th head and save CSV
# ------------------------------------------------------------

df_labels["clothing_label"] = df_labels["is_clothing"].astype(int)

os.makedirs(WASH_ROOT, exist_ok=True)
df_labels.to_csv(LABELS_CSV, index=False)
print("\nSaved labels CSV to:", LABELS_CSV)
print("Unique clothing_label values:", df_labels["clothing_label"].unique())

"""---

#### 6)  Label distribution analysis

In this stage of the pipeline, I loaded the unified metadata file `wash_labels.csv`, which contains all extracted labels for every cropped image. After successfully reading the file, I verified that the key supervisory columns—`is_clothing` and its direct copy `clothing_label`—were present. These fields are essential because they define the binary target for the 4th head of the multi-task network (clothing vs. non-clothing detection).

I then separated the dataset into two subsets:
- **Clothing (`is_clothing = 1`)**  
- **Non-clothing (`is_clothing = 0`)**

This allowed me to inspect how many images belonged to each category and to ensure that the dataset was not corrupted or mislabeled.

Once the split was confirmed, I focused on two primary attributes for clothing items:
1. **Color group** (LIGHT, DARK, COLOR, COLORFUL)  
2. **Fabric group** (COTTON, SYNTHETIC, WOOL, LINEN, SILK)

I computed frequency counts for each group. These counts were then visualized using clean, sorted bar plots. The charts showed very clear trends:

- **Color distribution**  
  COLORFUL garments appeared most frequently, followed by DARK and LIGHT groups, while plain COLOR items had the smallest representation.

- **Fabric distribution**  
  COTTON dominated the dataset by a large margin, SYNTHETIC was the second largest group, and materials like WOOL, LINEN, and SILK appeared far less frequently.

Finally, I analyzed the **global clothing vs. non-clothing balance**, which confirmed that the dataset contained significantly more clothing images than accessories. Because this binary task is used by the fourth classification head of the model, this imbalance is important to acknowledge before training.

Together, these visual checks ensured that:
- the label extraction pipeline was functioning correctly,  
- the dataset had the structure necessary for multi-head learning,  
- and the model would receive well-defined, reliable supervision for all four prediction tasks.

"""

# ============================================================
# Cell 6 — Load wash_labels.csv and visualize label distributions
#   • Uses is_clothing to split clothing vs non-clothing
#   • Plots distributions for color_group and fabric_group
#   • Prints is_clothing distribution (4th head target)
# ============================================================

import os
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt

# Core path
WASH_ROOT  = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV = os.path.join(WASH_ROOT, "wash_labels.csv")

print("Reading labels from:", LABELS_CSV)
assert os.path.exists(LABELS_CSV), f"wash_labels.csv not found at {LABELS_CSV}"

df_labels = pd.read_csv(LABELS_CSV)

# Ensure is_clothing exists
if "is_clothing" not in df_labels.columns:
    raise KeyError("'is_clothing' column not found in wash_labels.csv. "
                   "Please re-run the MASTER CELL that builds the labels.")

#  Define clothing_label explicitly as a copy of is_clothing
if "clothing_label" not in df_labels.columns:
    df_labels["clothing_label"] = df_labels["is_clothing"].astype(int)

# ------------------------------------------------------------
# Split into clothing and non-clothing subsets
# ------------------------------------------------------------
df_clothing    = df_labels[df_labels["is_clothing"] == 1]
df_nonclothing = df_labels[df_labels["is_clothing"] == 0]

print("\nHead of df_labels:")
print(df_labels.head())

print("\nSplit counts (all rows):")
print(df_labels["split"].value_counts())

print("\nClothing vs Non-clothing counts (is_clothing):")
print(df_labels["is_clothing"].value_counts())

print("\nClothing-label distribution (4th head target):")
print(df_labels["clothing_label"].value_counts())

print("\nColor group distribution (CLOTHING ONLY):")
print(df_clothing["color_group"].value_counts())

print("\nFabric group distribution (CLOTHING ONLY):")
print(df_clothing["fabric_group"].value_counts())


# --------- Helper: sorted bar plot ----------
def plot_sorted_bar(counts, title, xlabel, ylabel, color_map=None):
    counts = counts.sort_values(ascending=False)
    classes = counts.index.tolist()
    values  = counts.values

    colors = [color_map.get(c, None) for c in classes] if color_map else None

    plt.figure(figsize=(8, 5))
    plt.bar(classes, values, color=colors)
    plt.title(title, fontsize=14)
    plt.xlabel(xlabel, fontsize=12)
    plt.ylabel(ylabel, fontsize=12)
    plt.grid(axis="y", linestyle="--", alpha=0.4)
    plt.xticks(rotation=20)
    plt.tight_layout()
    plt.show()


# Color palette
color_group_palette = {
    "LIGHT":    "#f4e3b2",
    "DARK":     "#333333",
    "COLORFUL": "#ff7f50",
    "COLOR":    "#4682b4",
}

fabric_group_palette = {
    "COTTON":    "#66c2a5",
    "LINEN":     "#8da0cb",
    "WOOL":      "#fc8d62",
    "SILK":      "#e78ac3",
    "SYNTHETIC": "#a6d854",
}

# ------------------------------------------------------------
# Plot distributions for CLOTHING ONLY
# ------------------------------------------------------------
plot_sorted_bar(
    df_clothing["color_group"].value_counts(),
    title="Distribution of Color Groups — CLOTHING ONLY",
    xlabel="Color Group",
    ylabel="Count",
    color_map=color_group_palette,
)

plot_sorted_bar(
    df_clothing["fabric_group"].value_counts(),
    title="Distribution of Fabric Groups — CLOTHING ONLY",
    xlabel="Fabric Group",
    ylabel="Count",
    color_map=fabric_group_palette,
)

# ------------------------------------------------------------
# Visualize clothing vs non-clothing balance (4th head)
# ------------------------------------------------------------
clothing_counts = df_labels["is_clothing"].value_counts().sort_index()
# Assuming 0 = NON_CLOTHING, 1 = CLOTHING:
clothing_counts.index = ["NON_CLOTHING (0)", "CLOTHING (1)"]

plt.figure(figsize=(5, 4))
plt.bar(clothing_counts.index, clothing_counts.values)
plt.title("Clothing vs Non-clothing — All Images")
plt.ylabel("Count")
plt.grid(axis="y", linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()

"""---

## 7) Multi-Head Label Analysis

The dataset shows several positive characteristics that directly support training a stable and reliable 4-head model:

### ✔ Strong Clothing Representation  
The majority of images belong to clothing items, giving the model a solid foundation for learning garment-related features.

### ✔ Clear Color Patterns  
Color categories form well-defined groups, with COLORFUL, DARK, and LIGHT providing rich visual diversity for the color head.

### ✔ Well-Structured Fabric Categories  
Although cotton is dominant, all major fabric types appear in clean, consistent clusters. This helps the fabric head recognize material-dependent patterns.

### ✔ Meaningful Color × Fabric Relationships  
The heatmap revealed logical combinations (e.g., COLORFUL–COTTON), confirming that the dataset reflects real-world garment behavior, which benefits multi-attribute learning.

### ✔ Balanced Binary Clothing Labels  
The clothing vs non-clothing separation is clear and well-organized, making the binary head easy to learn and stable during training.

---

Overall, the label structure is clean, consistent, and rich in useful patterns—perfect for training a dependable multi-head washing-recommendation model.

"""

# ============================================================
# Cell 7 — Full Multi-Head Analysis (for 4-head model)
# ============================================================

import numpy as np
import matplotlib.pyplot as plt

# ------------------------------------------------------------
# Safety checks
# ------------------------------------------------------------
required_cols = [
    "color_group", "fabric_group",
    "color_label", "fabric_label",
    "is_clothing"
]

for c in required_cols:
    if c not in df_labels.columns:
        raise KeyError(f"Column {c} not found in df_labels!")

# ------------------------------------------------------------
# Subsets
# ------------------------------------------------------------
df_clothing    = df_labels[df_labels["is_clothing"] == 1]
df_nonclothing = df_labels[df_labels["is_clothing"] == 0]

print("Clothing count:", len(df_clothing))
print("Non-Clothing count:", len(df_nonclothing))

# ============================================================
# 1) HEATMAP: Color × Fabric (Clothing Only)
# ============================================================

pivot = (
    df_clothing
    .groupby(["color_group", "fabric_group"])
    .size()
    .unstack(fill_value=0)
)

pivot_percent = pivot / pivot.values.sum() * 100

fig, ax = plt.subplots(figsize=(10, 6))
im = ax.imshow(pivot.values, cmap="YlGnBu")

cbar = plt.colorbar(im)
cbar.ax.set_ylabel("Count", rotation=-90, va="bottom")

ax.set_xticks(np.arange(len(pivot.columns)))
ax.set_yticks(np.arange(len(pivot.index)))

ax.set_xticklabels(pivot.columns, fontsize=12)
ax.set_yticklabels(pivot.index, fontsize=12)

plt.title("Color × Fabric Distribution (Clothing Only)", fontsize=15, pad=15)
plt.setp(ax.get_xticklabels(), rotation=20, ha="right")

for i in range(len(pivot.index)):
    for j in range(len(pivot.columns)):
        count   = pivot.values[i, j]
        percent = pivot_percent.values[i, j]
        ax.text(
            j, i,
            f"{count}\n({percent:.1f}%)",
            ha="center", va="center",
            fontsize=9,
            color="black" if percent < 50 else "white",
        )

plt.tight_layout()
plt.show()

# ============================================================
# 2) COLOR HEAD DISTRIBUTION (clothing only)
# ============================================================

plt.figure(figsize=(6,4))
df_clothing["color_group"].value_counts().sort_values().plot(
    kind="barh", color=["#f4e3b2", "#333333", "#ff7f50", "#4682b4"]
)
plt.title("Color Head Distribution — Clothing Only")
plt.xlabel("Count")
plt.grid(axis="x", linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()

# ============================================================
# 3) FABRIC HEAD DISTRIBUTION (clothing only)
# ============================================================

plt.figure(figsize=(6,4))
df_clothing["fabric_group"].value_counts().sort_values().plot(
    kind="barh",
    color=["#66c2a5", "#8da0cb", "#fc8d62", "#e78ac3", "#a6d854"]
)
plt.title("Fabric Head Distribution — Clothing Only")
plt.xlabel("Count")
plt.grid(axis="x", linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()

# ============================================================
# 4) CLOTHING HEAD DISTRIBUTION (all images)
# ============================================================

cl_counts = df_labels["is_clothing"].value_counts().sort_index()
cl_counts.index = ["NON-CLOTHING (0)", "CLOTHING (1)"]

plt.figure(figsize=(5,4))
cl_counts.plot(kind="bar", color=["#a0a0a0", "#3c8dbc"])
plt.title("Clothing Head (Binary) Distribution — All Images")
plt.ylabel("Count")
plt.grid(axis="y", linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()

print("\n=== Summary ===")
print("Color classes:", df_clothing["color_group"].value_counts().to_dict())
print("Fabric classes:", df_clothing["fabric_group"].value_counts().to_dict())
print("Clothing binary:", df_labels["is_clothing"].value_counts().to_dict())

"""---
## 8) Random Sample Visualization

I used this step to visually inspect random samples from my dataset and confirm that all
labels were assigned correctly.  
For clothing images, I checked the **color group**, **fabric group**, and the **wash-cycle** annotations.  
For non-clothing items, I verified that each sample was correctly marked as
`is_clothing = 0`.

This visual examination helped me ensure that the dataset was clean, consistent, and ready
for training the multi-head model.

"""

# ============================================================
# Cell 8 — Visualize Random Samples (Clothing + Non-Clothing)
# Fully compatible with 4-head model:
#   • color_head        → color_label / color_group
#   • fabric_head       → fabric_label / fabric_group
#   • wash_cycle_head   → wash_cycle_label   (if available)
#   • is_clothing_head  → is_clothing (0 = NON_CLOTHING, 1 = CLOTHING)
# ============================================================

import random
from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt

assert "df_labels" in globals(),      "df_labels not loaded."
assert "df_clothing" in globals(),    "df_clothing not loaded."
assert "df_nonclothing" in globals(), "df_nonclothing not loaded."


# ------------------------------------------------------------
# Helper
# ------------------------------------------------------------
def show_random_labeled_samples(
    df,
    title,
    n=8,
    show_color_fabric=True,
    show_wash_cycle=True,
):
    """
    Visualize random samples with labels corresponding to the 4-head setup.

    If show_color_fabric = True:
        - Displays color_group, fabric_group
        - If wash_cycle_label exists and show_wash_cycle=True:
              also shows wash_cycle_label
        - Always shows is_clothing (should be 1 for clothing subset)

    If show_color_fabric = False:
        - Used for NON-CLOTHING samples:
              shows filename + 'NON_CLOTHING (is_clothing=0)'.
    """
    if len(df) == 0:
        print(f"No samples for: {title}")
        return

    k = min(n, len(df))
    subset = df.sample(k, random_state=42)

    cols = 4
    rows = (k + cols - 1) // cols
    plt.figure(figsize=(4 * cols, 4 * rows))

    has_wash = ("wash_cycle_label" in subset.columns) and show_wash_cycle

    for i, (_, row) in enumerate(subset.iterrows()):
        img_path = row["image_path"]

        try:
            img = Image.open(img_path).convert("RGB")
        except Exception as e:
            print(f"Error loading {img_path}: {e}")
            continue

        plt.subplot(rows, cols, i + 1)
        plt.imshow(img)
        plt.axis("off")

        is_clothing_val = int(row.get("is_clothing", -1))

        if show_color_fabric:
            # Clothing sample: show color/fabric (wash_cycle)
            color_txt  = str(row.get("color_group", "NA"))
            fabric_txt = str(row.get("fabric_group", "NA"))
            wash_txt   = str(row.get("wash_cycle_label", "NA")) if has_wash else "NA"

            title_text = (
                f"{Path(img_path).name}\n"
                f"Color: {color_txt} | Fabric: {fabric_txt}\n"
            )
            if has_wash:
                title_text += f"Wash: {wash_txt}\n"

            title_text += f"is_clothing = {is_clothing_val} | article_id: {row['article_id']}"
        else:
            # Non-clothing sample: negative class for 4th head
            title_text = (
                f"{Path(img_path).name}\n"
                f"NON_CLOTHING (is_clothing = {is_clothing_val})\n"
                f"article_id: {row['article_id']}"
            )

        plt.title(title_text, fontsize=8)

    plt.suptitle(title, fontsize=14)
    plt.tight_layout()
    plt.show()


# ------------------------------------------------------------
# Part A — Visualize CLOTHING samples by color_group and fabric_group
#   (heads: color_head, fabric_head, and(wash_cycle_head)
# ------------------------------------------------------------

for cg in ["LIGHT", "DARK", "COLORFUL", "COLOR"]:
    df_cg = df_clothing[
        (df_clothing["split"] == "train") &
        (df_clothing["color_group"] == cg)
    ]
    print(f"\n=== Clothing — color_group = {cg} (train) ===")
    print("Count:", len(df_cg))
    show_random_labeled_samples(
        df_cg,
        title=f"TRAIN — CLOTHING — color_group = {cg}",
        n=8,
        show_color_fabric=True,
        show_wash_cycle=True,
    )

for fg in ["COTTON", "LINEN", "WOOL", "SILK", "SYNTHETIC"]:
    df_fg = df_clothing[
        (df_clothing["split"] == "train") &
        (df_clothing["fabric_group"] == fg)
    ]
    if len(df_fg) == 0:
        continue
    print(f"\n=== Clothing — fabric_group = {fg} (train) ===")
    print("Count:", len(df_fg))
    show_random_labeled_samples(
        df_fg,
        title=f"TRAIN — CLOTHING — fabric_group = {fg}",
        n=8,
        show_color_fabric=True,
        show_wash_cycle=True,
    )


# ------------------------------------------------------------
# Part B — Visualize NON-CLOTHING ACCESSORY SAMPLES
#   (4th head: is_clothing_head, target = 0)
# ------------------------------------------------------------

print("\n====================================================")
print("          NON-CLOTHING / ACCESSORY SAMPLES")
print("              (Negative class for 4th head)")
print("====================================================")
print("Count:", len(df_nonclothing))

show_random_labeled_samples(
    df_nonclothing,
    title="TRAIN — NON-CLOTHING (Accessories / Negative Class for is_clothing)",
    n=12,
    show_color_fabric=False,   # No color/fabric labels for accessories
    show_wash_cycle=False,
)

"""---
## 9)Post-processing of `wash_labels.csv`

In this stage, I performed two important cleaning steps to improve the quality of my
labels before training the 4-head model.  
My goal was to refine color annotations for very light clothing items and to remove any
multi-pack articles that could introduce ambiguity.

### Refining LIGHT vs COLORFUL  
I corrected cases where clothing items were labeled *COLORFUL* even though their image
statistics indicated that they were actually very light.  
By checking the **HSV brightness (v)** and **saturation (s)**, I reclassified items with:

- high brightness,  
- low saturation,  
- and originally marked as COLORFUL,  

into the **LIGHT** category.  
This fixed a large batch of mislabeled samples and gave me cleaner color groups for the
model.

### Removing Multi-Pack Items  
Using H&M metadata, I filtered out all articles that belonged to **multi-packs**, because
those products often contain several garments with different colors or fabrics.  
Removing them helped ensure that each image corresponds to one coherent item, which is
important for correct labeling in a multi-head architecture.

### Final Result  
After these corrections, my dataset became cleaner, more consistent, and more reliable for
training the final version of my model.

"""

# ============================================================
# Cell 9 — Post-processing for wash_labels.csv (4-head aware)
# ------------------------------------------------------------
# This cell performs two clean-up steps on the label file:
#   (A) Refine LIGHT vs COLORFUL for very light CLOTHING items
#   (B) Remove multi-pack (multi-garment) articles using H&M metadata
#
# 4-head consistency:
#   • is_clothing_head  → is_clothing (0 = NON_CLOTHING, 1 = CLOTHING)
#   • Only CLOTHING rows (is_clothing == 1) get a valid color_label.
#   • NON-CLOTHING rows (is_clothing == 0) keep their NaN color_label.
#   • If 'clothing_label' exists, it is kept aligned with is_clothing.
#   • The cleaned file is saved back to wash_labels.csv for training.
# ============================================================

import os
import re
import pandas as pd

# --------------------------------------------------------------------
# 0) Paths (must be consistent with previous cells)
# --------------------------------------------------------------------
WASH_ROOT    = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV   = os.path.join(WASH_ROOT, "wash_labels.csv")
ARTICLES_CSV = "/content/drive/MyDrive/hm/articles.csv"

print("Loading labels from:", LABELS_CSV)
df_labels = pd.read_csv(LABELS_CSV)

# --- Sanity checks for 4-head setup ---
if "is_clothing" not in df_labels.columns:
    raise KeyError(
        "Column 'is_clothing' not found in wash_labels.csv — "
        "run the MASTER LABEL-BUILDER cell first."
    )

# Keep 'clothing_label' synchronized with 'is_clothing'
if "clothing_label" in df_labels.columns:
    df_labels["clothing_label"] = df_labels["is_clothing"].astype(int)

print("\nOriginal color_group counts (all rows, including non-clothing):")
print(df_labels["color_group"].value_counts(dropna=False))

# Consistent mapping used everywhere in this notebook
COLOR2IDX = {
    "LIGHT":    0,
    "DARK":     1,
    "COLORFUL": 2,
    "COLOR":    3,
}

# --------------------------------------------------------------------
# (A) Refine LIGHT vs COLORFUL — CLOTHING ONLY
# --------------------------------------------------------------------
# Rule:
#   • Only for CLOTHING rows  (is_clothing == 1)
#   • Currently labeled as COLORFUL
#   • Very light (v >= 0.75)
#   • Not strongly saturated (s <= 0.65)
# These are relabeled from COLORFUL → LIGHT.

mask_clothing = (df_labels["is_clothing"] == 1)
mask_colorful = (df_labels["color_group"] == "COLORFUL")
mask_lightish = (df_labels["v"] >= 0.75) & (df_labels["s"] <= 0.65)

mask_to_light = mask_clothing & mask_colorful & mask_lightish

n_to_light = int(mask_to_light.sum())
print(
    f"\nRelabeling {n_to_light} CLOTHING samples from "
    f"COLORFUL → LIGHT (v >= 0.75 and s <= 0.65)."
)

df_labels.loc[mask_to_light, "color_group"] = "LIGHT"

# Recompute numeric color_label ONLY for clothing rows
df_labels.loc[mask_clothing, "color_label"] = (
    df_labels.loc[mask_clothing, "color_group"].map(COLOR2IDX)
)

print("\nColor_group counts AFTER LIGHT/COLORFUL fix (all rows):")
print(df_labels["color_group"].value_counts(dropna=False))

# Sanity: non-clothing rows keep NaN color_label
if "color_label" in df_labels.columns:
    nonclothing_mask = (df_labels["is_clothing"] == 0)
    n_noncloth_non_nan = df_labels.loc[nonclothing_mask, "color_label"].notna().sum()
    if n_noncloth_non_nan > 0:
        print(f"\n[Warning] Found {n_noncloth_non_nan} non-clothing rows "
              "with non-NaN color_label. You may want to reset them to NaN.")
        # If you want to enforce NaN for non-clothing explicitly, uncomment:
        # df_labels.loc[nonclothing_mask, "color_label"] = pd.NA

# --------------------------------------------------------------------
# (B) Remove multi-pack / multi-garment articles using articles.csv
# --------------------------------------------------------------------
print("\nLoading articles.csv to detect multi-pack items ...")
articles = pd.read_csv(
    ARTICLES_CSV,
    dtype={"article_id": str},
    low_memory=False,
)

# Standardize text columns to lower-case
for col in ["prod_name", "detail_desc"]:
    if col in articles.columns:
        articles[col] = articles[col].astype(str).str.lower()
    else:
        articles[col] = ""

# Heuristic: any occurrence of "pack" → treat as multi-pack item
articles["is_pack"] = (
    articles["prod_name"].str.contains("pack", na=False) |
    articles["detail_desc"].str.contains("pack", na=False)
)

pack_ids = set(articles.loc[articles["is_pack"], "article_id"])
print(f"Total article_ids flagged as multi-pack: {len(pack_ids)}")

mask_pack = df_labels["article_id"].astype(str).isin(pack_ids)
n_pack = int(mask_pack.sum())
print(f"Detected {n_pack} labeled rows belonging to multi-pack articles.")

# Drop multi-pack rows (both clothing and non-clothing) to keep labels clean
df_labels_clean = df_labels[~mask_pack].reset_index(drop=True)

print("\nFinal color_group counts AFTER removing multi-pack rows (all rows):")
print(df_labels_clean["color_group"].value_counts(dropna=False))

print("\nRemaining rows after cleaning:", len(df_labels_clean))

# --------------------------------------------------------------------
# Save cleaned labels back to CSV
# --------------------------------------------------------------------
df_labels_clean.to_csv(LABELS_CSV, index=False)
print("\nSaved cleaned labels back to:", LABELS_CSV)

"""---
## 10) Recomputing Color Groups for Clothing

In this step, I refined **color_group** only for clothing images to improve the
quality of the color head.

### What I updated
- **Patterned items (`pattern_flag = 1`)**  
  → set to `COLORFUL` (prints, checks, stripes, etc.).

- **Plain items (`pattern_flag = 0`)**  
  Classified with HSV:
  - Low brightness → **DARK**
  - Very light + low saturation → **LIGHT**
  - Otherwise → **COLOR**

### Why this helps
This correction makes the color labels more consistent with real garment
appearance and gives the model cleaner, more meaningful targets.

### Additional consistency
- Only `is_clothing = 1` rows were modified.
- Non-clothing rows remain untouched.
- `color_label` was recomputed to match updated `color_group`.

This step improved the semantic accuracy of the color head without affecting the
other model outputs. ```

"""

# ============================================================
# Cell 10 — Recompute color_group for CLOTHING (plain vs patterned)
# ------------------------------------------------------------
# 4-head aware:
#   • is_clothing_head  → is_clothing (0 = NON_CLOTHING, 1 = CLOTHING)
#   • color_head        → color_label / color_group  (CLOTHING ONLY)
#   • fabric_head       → fabric_label / fabric_group (unchanged here)
#   • wash_cycle_head   → wash_cycle_label (if present, unchanged here)
#
# Goal in this cell:
#   • Patterned clothing (pattern_flag = 1) → COLORFUL
#   • Plain clothing (pattern_flag = 0)     → DARK / LIGHT / COLOR (via HSV)
#
# Notes:
#   • Only CLOTHING rows (is_clothing == 1) are modified.
#   • Non-clothing rows (is_clothing == 0) are left untouched.
#   • color_label is recomputed only for clothing rows using COLOR2IDX.
#   • If 'clothing_label' exists, it remains aligned with is_clothing.
# ============================================================

import os
import pandas as pd

WASH_ROOT  = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV = os.path.join(WASH_ROOT, "wash_labels.csv")

print("Loading labels from:", LABELS_CSV)
df_labels = pd.read_csv(LABELS_CSV)

# --- Sanity checks for 4-head setup ---
if "is_clothing" not in df_labels.columns:
    raise KeyError(
        "Column 'is_clothing' not found in wash_labels.csv — "
        "run the master label-builder cell first."
    )

# Keep 'clothing_label' synchronized with 'is_clothing'
if "clothing_label" in df_labels.columns:
    df_labels["clothing_label"] = df_labels["is_clothing"].astype(int)

print("\nOriginal color_group counts (all rows, including non-clothing):")
print(df_labels["color_group"].value_counts(dropna=False))

# Consistent mapping used throughout the notebook
COLOR2IDX = {
    "LIGHT":    0,
    "DARK":     1,
    "COLORFUL": 2,
    "COLOR":    3,
}

# -----------------------------
# HSV thresholds (tunable)
# -----------------------------
THRESH_DARK_V  = 0.55   # v ≤ 0.55  → DARK  (plain)
THRESH_LIGHT_V = 0.75   # v ≥ 0.75 and s ≤ 0.65 → LIGHT
THRESH_LIGHT_S = 0.65

# Work ONLY on clothing rows
mask_clothing = (df_labels["is_clothing"] == 1)

# Convenience aliases
if "pattern_flag" not in df_labels.columns:
    raise KeyError("Column 'pattern_flag' not found in wash_labels.csv.")
if "v" not in df_labels.columns or "s" not in df_labels.columns:
    raise KeyError("HSV columns 'v' and 's' are required for this cell.")

pattern_flag = df_labels["pattern_flag"]
v = df_labels["v"]
s = df_labels["s"]

mask_pattern = mask_clothing & (pattern_flag == 1)
mask_plain   = mask_clothing & (pattern_flag == 0)

plain_dark  = mask_plain & (v <= THRESH_DARK_V)
plain_light = mask_plain & (v >= THRESH_LIGHT_V) & (s <= THRESH_LIGHT_S)
plain_color = mask_plain & ~(plain_dark | plain_light)

# 1) Patterned clothing → COLORFUL
df_labels.loc[mask_pattern, "color_group"] = "COLORFUL"

# 2) Plain clothing → DARK / LIGHT / COLOR
df_labels.loc[plain_dark,  "color_group"] = "DARK"
df_labels.loc[plain_light, "color_group"] = "LIGHT"
df_labels.loc[plain_color, "color_group"] = "COLOR"

# Recompute numeric color_label ONLY for clothing rows
df_labels.loc[mask_clothing, "color_label"] = (
    df_labels.loc[mask_clothing, "color_group"].map(COLOR2IDX)
)

print("\nUpdated color_group counts (all rows, including non-clothing):")
print(df_labels["color_group"].value_counts(dropna=False))

# Sanity: non-clothing rows should still have NaN color_label (if you enforce that convention)
if "color_label" in df_labels.columns:
    nonclothing_mask = (df_labels["is_clothing"] == 0)
    n_noncloth_non_nan = df_labels.loc[nonclothing_mask, "color_label"].notna().sum()
    if n_noncloth_non_nan > 0:
        print(f"\n[Warning] Found {n_noncloth_non_nan} non-clothing rows with non-NaN color_label.")
        # Optionally enforce NaN for non-clothing:
        # df_labels.loc[nonclothing_mask, "color_label"] = pd.NA

df_labels.to_csv(LABELS_CSV, index=False)
print("\nSaved updated labels to:", LABELS_CSV)

"""---
## 11) Visualizing Clothing Images by Color Group

In this step, I focused only on **clothing items** (`is_clothing = 1`) and created
a clean visual summary of how garments look inside each **color_group** category.

### What I did
- Loaded the final `wash_labels.csv`.
- Filtered out all non-clothing items.
- For each color group (**LIGHT**, **DARK**, **COLOR**, **COLORFUL**),  
  I displayed a small set of random garment images.

### What each sample shows
For every image, I included:
- File name  
- `article_id`  
- `color_group`  
- `fabric_group`  
-  `wash_cycle_label`  
- `is_clothing`

### Why I visualized this
These previews helped me verify:
- The color grouping rules worked correctly  
- Patterned vs. plain logic behaved as expected  
- Garments inside each group look visually consistent  

This step gave me a final sanity-check to ensure that the **color head** of my
multitask model receives clean and meaningful image-label pairs.

"""

# ============================================================
# Cell 11 — Visualize CLOTHING images by COLOR GROUP (4-head aware)
# ------------------------------------------------------------
# 4-head interpretation:
#   • color_head        → color_group / color_label   (used here)
#   • fabric_head       → fabric_group / fabric_label (shown in titles)
#   • wash_cycle_head   → wash_cycle_label
#   • is_clothing_head  → is_clothing (0/1) — used to restrict to CLOTHING ONLY
#
# This cell:
#   • Loads wash_labels.csv
#   • Restricts to CLOTHING rows (is_clothing == 1)
#   • For each color_group, displays random samples with:
#       - filename
#       - article_id
#       - color_group / fabric_group
#       - wash_cycle_label
#       - is_clothing (should be 1 for all shown samples)
# ============================================================

import os
import random
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image

# --- Load labels ---
WASH_ROOT  = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV = os.path.join(WASH_ROOT, "wash_labels.csv")

print("Reading labels from:", LABELS_CSV)
df_labels = pd.read_csv(LABELS_CSV)
print("Loaded", len(df_labels), "rows.")
print("Columns:", list(df_labels.columns))

# Basic sanity checks for 4-head setup
required_cols = ["image_path", "color_group", "fabric_group", "is_clothing"]
for col in required_cols:
    assert col in df_labels.columns, f"Missing required column: {col}"

# Keep 'clothing_label' synchronized with 'is_clothing'
if "clothing_label" in df_labels.columns:
    df_labels["clothing_label"] = df_labels["is_clothing"].astype(int)

# Work on CLOTHING ONLY
df_clothing = df_labels[df_labels["is_clothing"] == 1].copy()
n_non_clothing = (df_labels["is_clothing"] == 0).sum()

print("\nClothing rows    :", len(df_clothing))
print("Non-clothing rows:", n_non_clothing)

has_wash_cycle = "wash_cycle_label" in df_clothing.columns

# --- Helper: safe image loader ---
def safe_open(path: str):
    try:
        return Image.open(path).convert("RGB")
    except Exception as e:
        print(f"[WARN] Could not open {path}: {e}")
        return None

# --- Helper: show N random samples for a given color_group (CLOTHING ONLY) ---
def show_samples_for_group(color_group: str, n: int = 9):
    subset = df_clothing[df_clothing["color_group"] == color_group]
    count = len(subset)
    print(f"\n=== {color_group} — available CLOTHING samples: {count} ===")

    if count == 0:
        print("No clothing samples for this group.")
        return

    k = min(n, count)
    subset = subset.sample(k, random_state=42)

    cols = 3
    rows = (k + cols - 1) // cols
    plt.figure(figsize=(4 * cols, 4 * rows))

    for i, row in enumerate(subset.itertuples(), start=1):
        img_path   = row.image_path
        color_g    = row.color_group
        fabric_g   = row.fabric_group
        article_id = getattr(row, "article_id", "N/A")
        is_cloth   = int(getattr(row, "is_clothing", -1))
        wash_txt   = None

        if has_wash_cycle:
            wash_txt = getattr(row, "wash_cycle_label", "NA")

        img = safe_open(img_path)
        if img is None:
            continue

        ax = plt.subplot(rows, cols, i)
        ax.imshow(img)
        ax.axis("off")

        title_lines = [
            os.path.basename(img_path),
            f"Color: {color_g} | Fabric: {fabric_g}",
        ]
        if has_wash_cycle:
            title_lines.append(f"Wash: {wash_txt}")
        title_lines.append(f"is_clothing = {is_cloth} | article_id: {article_id}")

        ax.set_title("\n".join(title_lines), fontsize=8)

    plt.suptitle(f"CLOTHING — color_group = {color_group}", fontsize=14)
    plt.tight_layout()
    plt.show()

# --- Visualize by color_group (CLOTHING ONLY) ---
for cg in ["LIGHT", "DARK", "COLOR", "COLORFUL"]:
    show_samples_for_group(cg, n=9)

"""---
## 12) Rule-based Washing Program for Clothing

In this step I enriched `wash_labels.csv` with a **washing program** for **clothing rows only** (`is_clothing = 1`).

- For each clothing item, I used its **color_group** and **fabric_group** to assign:
  - `wash_temp_c` (°C)
  - `wash_cycle` ∈ {delicate, normal, heavy, quick, wool}
  - `wash_cycle_label` (integer code for the 3rd head)
  - `spin_rpm`
  - `wash_notes` (short care instructions)

- **Non-clothing rows** keep `NaN` / `"N/A"` in these columns, so they don’t interfere with the wash-cycle head.

This makes the 3rd head a **supervised, interpretable wash-cycle classifier**, directly aligned with fabric type and color safety rules.  

"""

# ============================================================
# Cell 12 — Add Washing Program Recommendation to wash_labels.csv
#              (CLOTHING rows only: is_clothing == 1)
#
# 4-head interpretation:
#   • color_head        → color_group / color_label        (unchanged here)
#   • fabric_head       → fabric_group / fabric_label      (unchanged here)
#   • wash_cycle_head   → wash_cycle_label (NEW, numeric)
#   • is_clothing_head  → is_clothing (0/1)                (unchanged here)
#
# This cell produces, for CLOTHING rows:
#   - wash_temp_c   : recommended temperature in °C
#   - wash_cycle    : one of {delicate, normal, heavy, quick, wool}
#   - wash_cycle_label : integer code for wash_cycle (for 3rd head)
#   - spin_rpm      : recommended spin speed
#   - wash_notes    : short textual guidance
#
# Non-clothing rows keep NaN / "N/A" for these washing columns.
# ============================================================

import os
import pandas as pd
import numpy as np

WASH_ROOT  = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV = os.path.join(WASH_ROOT, "wash_labels.csv")

df = pd.read_csv(LABELS_CSV)
print("Loaded labels:", len(df))
print("Columns:", list(df.columns))

# --- Sanity checks for 4-head setup ---
assert "color_group"  in df.columns,  "Missing 'color_group' in labels."
assert "fabric_group" in df.columns,  "Missing 'fabric_group' in labels."
assert "is_clothing"  in df.columns, \
    "Column 'is_clothing' not found — run the master label-builder first."

# Keep 'clothing_label' synchronized with 'is_clothing'
if "clothing_label" in df.columns:
    df["clothing_label"] = df["is_clothing"].astype(int)

# ------------------------------------------------------------
# Mapping for wash_cycle_head (3rd head)
# ------------------------------------------------------------
WASHCYCLE2IDX = {
    "delicate": 0,
    "normal":   1,
    "heavy":    2,
    "quick":    3,
    "wool":     4,
}

def assign_wash_program(color_group: str, fabric_group: str):
    """
    Rule-based washing program based on (color_group, fabric_group).

    Returns:
        temp_c   : recommended temperature (°C)
        cycle    : one of {delicate, normal, heavy, quick, wool}
        spin_rpm : recommended spin speed (rpm)
        notes    : short textual guidance
    """
    cg = str(color_group).upper()
    fg = str(fabric_group).upper()

    # -------- Defaults (safe baseline) --------
    temp_c   = 30
    cycle    = "normal"
    spin_rpm = 800
    notes    = []

    # -------- Fabric-driven rules --------
    if fg == "WOOL":
        temp_c   = 20
        cycle    = "wool"
        spin_rpm = 600
        notes.append("Use wool detergent if possible.")
        notes.append("Do not tumble dry.")
    elif fg == "SILK":
        temp_c   = 20
        cycle    = "delicate"
        spin_rpm = 400
        notes.append("Use mild detergent.")
        notes.append("Place in a laundry bag.")
    elif fg == "LINEN":
        temp_c   = 30
        cycle    = "normal"
        spin_rpm = 800
        notes.append("Avoid high heat to reduce shrinkage.")
    elif fg == "COTTON":
        temp_c   = 30
        cycle    = "normal"
        spin_rpm = 1000
        notes.append("Turn inside out if printed or dark.")
    elif fg == "SYNTHETIC":
        temp_c   = 30
        cycle    = "quick"
        spin_rpm = 1000
        notes.append("Use easy-care / anti-crease program if available.")

    # -------- Color-driven adjustments --------
    if cg == "DARK":
        temp_c   = min(temp_c, 30)
        spin_rpm = min(spin_rpm, 800)
        notes.append("Wash with similar dark colors.")
        notes.append("Use color-care detergent.")
    elif cg == "LIGHT":
        notes.append("Wash with light colors.")
    elif cg == "COLORFUL":
        temp_c   = min(temp_c, 30)
        spin_rpm = min(spin_rpm, 900)
        notes.append("Wash with similar bright colors.")
        notes.append("Do not overload the drum.")
    elif cg == "COLOR":
        temp_c = min(max(temp_c, 30), 40)
        notes.append("Wash with similar colors.")

    # Remove duplicate notes (preserve order)
    notes = list(dict.fromkeys(notes))

    return temp_c, cycle, spin_rpm, " ".join(notes)


# ------------------------------------------------------------
# Apply ONLY to clothing rows
# ------------------------------------------------------------

n = len(df)
wash_temp_c       = np.full(n, np.nan, dtype=float)
wash_cycle        = np.full(n, "N/A", dtype=object)
wash_cycle_label  = np.full(n, np.nan, dtype=float)  # numeric label for 3rd head
spin_rpm          = np.full(n, np.nan, dtype=float)
wash_notes        = np.full(n, "N/A", dtype=object)

mask_clothing = (df["is_clothing"] == 1)

for idx, row in df[mask_clothing].iterrows():
    t, c, s, nt = assign_wash_program(row["color_group"], row["fabric_group"])
    wash_temp_c[idx]      = t
    wash_cycle[idx]       = c
    spin_rpm[idx]         = s
    wash_notes[idx]       = nt
    # map to integer label for 3rd head
    wash_cycle_label[idx] = WASHCYCLE2IDX.get(c, np.nan)

df["wash_temp_c"]      = wash_temp_c
df["wash_cycle"]       = wash_cycle
df["wash_cycle_label"] = wash_cycle_label
df["spin_rpm"]         = spin_rpm
df["wash_notes"]       = wash_notes

print("\nSample rows with washing program (CLOTHING ONLY):")
print(
    df.loc[mask_clothing, [
        "image_path",
        "color_group",
        "fabric_group",
        "wash_temp_c",
        "wash_cycle",
        "wash_cycle_label",
        "spin_rpm",
        "wash_notes",
    ]].head()
)

df.to_csv(LABELS_CSV, index=False)
print("\nSaved updated wash_labels.csv with washing program columns and wash_cycle_label.")

"""---

## 13) Building the Final `wash_cycle_label` (4-Head Model Consistency)

In this step I generated the **final numeric wash-cycle label** used by the third head of the multi-task model.

- Each clothing item already had a textual `wash_cycle` assigned (e.g., *normal*, *quick*, *delicate*, *wool*).
- I converted these textual cycles into integer codes using a **fixed mapping**:

  - delicate → 0  
  - normal   → 1  
  - heavy    → 2  
  - quick    → 3  
  - wool     → 4  

- **Only clothing rows** (`is_clothing = 1`) receive a valid numeric label.  
- **Non-clothing samples** keep `NaN`, which avoids affecting the wash-cycle head during training.

This ensures the wash-cycle head is fully synchronized with the labels created earlier, giving a clean and consistent target for training the 4-head deep learning model.

"""

# ============================================================
# Cell 13 — Build wash_cycle_label (consistent with 4-head model)
#
# 4-head:
#   color_head        → color_label
#   fabric_head       → fabric_label
#   wash_cycle_head   → wash_cycle_label (THIS CELL)
#   is_clothing_head  → is_clothing
#
# Notes:
#   • Only CLOTHING rows receive a valid wash_cycle_label.
#   • NON-CLOTHING rows get NaN (easier for masking in training).
#   • Mapping is FIXED and MATCHES Cell 13.
# ============================================================

import os
import pandas as pd
import numpy as np

WASH_ROOT  = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV = os.path.join(WASH_ROOT, "wash_labels.csv")

df = pd.read_csv(LABELS_CSV)
print("Loaded:", len(df), "rows")
print("Columns:", list(df.columns))

assert "wash_cycle"  in df.columns,  "Missing wash_cycle — run Cell 13 first."
assert "is_clothing" in df.columns,  "Missing is_clothing."

# ------------------------------------------------------------
# FIXED mapping for the 4-head wash_cycle_head
# ------------------------------------------------------------

WASHCYCLE2IDX = {
    "delicate": 0,
    "normal":   1,
    "heavy":    2,
    "quick":    3,
    "wool":     4,
}

IDX2WASHCYCLE = {v: k for k, v in WASHCYCLE2IDX.items()}

# ------------------------------------------------------------
# Encode wash_cycle_label (CLOTHING ONLY)
# ------------------------------------------------------------

wash_cycle_label = np.full(len(df), np.nan, dtype=float)  # NaN = non-clothing

mask_clothing = (df["is_clothing"] == 1)

for idx, row in df.loc[mask_clothing].iterrows():
    cycle = str(row["wash_cycle"]).lower().strip()
    if cycle in WASHCYCLE2IDX:
        wash_cycle_label[idx] = WASHCYCLE2IDX[cycle]
    else:
        print(f"[Warning] Unknown wash_cycle '{cycle}' at index {idx}")

df["wash_cycle_label"] = wash_cycle_label

print("\nDistribution of wash_cycle_label (NaN = non-clothing):")
print(df["wash_cycle_label"].value_counts(dropna=False))

df.to_csv(LABELS_CSV, index=False)
print("\nSaved updated CSV with wash_cycle_label.")
print("Mapping:", WASHCYCLE2IDX)

"""---

## 14) Multi-Task ConvNeXt Training (4-Head Model)

In this stage, I trained a **multi-task ConvNeXt-Tiny model** designed to learn four outputs simultaneously:

- **Color classification**  
- **Fabric classification**  
- **Wash-cycle recommendation**  
- **Clothing vs. non-clothing detection**

All images (both clothing and non-clothing) were included in the dataset.  
  
This allowed the model to learn clean, stable boundaries without noise from accessory images.

- A single ConvNeXt backbone efficiently learned four tasks at once.  
- A smart masking strategy ensured that non-clothing samples never corrupted color, fabric, or wash-cycle learning.  
- Loss weights were balanced to stabilize multi-task training.  
- Automatic checkpointing saved the best model based on wash-cycle validation accuracy.

### Final Performance
- **Color accuracy:** ~82%  
- **Fabric accuracy:** ~80%  
- **Wash-cycle accuracy:** ~83%  
- **Is-clothing accuracy:** ~94%  

The best model achieved **83.07% wash-cycle accuracy** and was saved automatically.

"""

# ============================================================
# Cell 14 — Multi-Task ConvNeXt for
#   COLOR + FABRIC + WASH CYCLE + IS_CLOTHING  (4-head model)
# ============================================================

import os
import time
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import timm
import numpy as np
import pandas as pd
from tqdm.auto import tqdm

# -------------------------------------------------------------
# 0) Configuration and Label Loading
# -------------------------------------------------------------

WASH_ROOT   = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV  = os.path.join(WASH_ROOT, "wash_labels.csv")
OUTPUT_DIR  = os.path.join(WASH_ROOT, "outputs_multitask_wash")

os.makedirs(OUTPUT_DIR, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

if timm is None:
    raise ImportError("timm is required for ConvNeXt. Please install timm first.")

df_labels = pd.read_csv(LABELS_CSV)
print("Loaded", len(df_labels), "rows from CSV")
print("Columns:", list(df_labels.columns))

required_cols = [
    "image_path",
    "split",
    "color_label",
    "fabric_label",
    "wash_cycle_label",
    "color_group",
    "fabric_group",
    "wash_cycle",
    "is_clothing",
]
for c in required_cols:
    assert c in df_labels.columns, f"Missing column: {c}"

# -------------------------------------------------------------
# IMPORTANT: Do NOT drop non-clothing rows
#   - We need them for the 4th head: is_clothing
# -------------------------------------------------------------
df_labels["is_clothing"] = df_labels["is_clothing"].astype(int)

# Keep clothing_label in sync if it exists
if "clothing_label" in df_labels.columns:
    df_labels["clothing_label"] = df_labels["is_clothing"].astype(int)

# For non-clothing rows, color/fabric/wash labels may be NaN.
# We only use their losses for clothing samples (masking),
# so here we make dummy integer labels to keep tensor dtypes consistent.
df_labels["color_label_train"] = df_labels["color_label"].fillna(0).astype(int)
df_labels["fabric_label_train"] = df_labels["fabric_label"].fillna(0).astype(int)
df_labels["wash_cycle_label_train"] = df_labels["wash_cycle_label"].fillna(0).astype(int)

# Number of classes for each head — based on MAX label (clothing only),
# assuming contiguous labels {0, ..., C-1} from previous mapping.
mask_cloth = (df_labels["is_clothing"] == 1)

num_color_classes  = int(df_labels.loc[mask_cloth, "color_label"].max()) + 1
num_fabric_classes = int(df_labels.loc[mask_cloth, "fabric_label"].max()) + 1
num_wash_classes   = int(df_labels.loc[mask_cloth, "wash_cycle_label"].max()) + 1
num_iscloth_classes = 2  # {0: NON_CLOTHING, 1: CLOTHING}

print(f"Num COLOR classes      : {num_color_classes}")
print(f"Num FABRIC classes     : {num_fabric_classes}")
print(f"Num WASH_CYCLE classes : {num_wash_classes}")
print(f"Num IS_CLOTHING classes: {num_iscloth_classes}")

# -------------------------------------------------------------
# Ensure a non-empty TRAIN split; regenerate if needed
# -------------------------------------------------------------
if "split" not in df_labels.columns:
    print("WARNING: 'split' column missing → assigning all rows to TRAIN.")
    df_labels["split"] = "train"
else:
    n_train = (df_labels["split"] == "train").sum()
    n_val   = (df_labels["split"] == "val").sum()
    print(f"Split (before fix): train={n_train}, val={n_val}")

    if n_train == 0:
        print("WARNING: no TRAIN rows found → creating new 80/20 split.")
        df_labels = df_labels.sample(frac=1.0, random_state=42).reset_index(drop=True)

        n_total = len(df_labels)
        n_train_new = int(0.80 * n_total)

        df_labels.loc[:n_train_new-1, "split"] = "train"
        df_labels.loc[n_train_new:,   "split"] = "val"

        df_labels.to_csv(LABELS_CSV, index=False)
        print(f"New split saved: TRAIN={n_train_new}, VAL={n_total - n_train_new}")

# -------------------------------------------------------------
# 1) Dataset and Transformations
# -------------------------------------------------------------

IMG_SIZE = 256
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD  = [0.229, 0.224, 0.225]

train_transform = transforms.Compose([
    transforms.Resize(IMG_SIZE + 32),
    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.03),
    transforms.ToTensor(),
    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
])

val_transform = transforms.Compose([
    transforms.Resize(IMG_SIZE + 32),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
])


class WashMultiTaskDataset(Dataset):
    """
    4-head dataset:

        X           → image tensor
        y_color     → class index in [0, num_color_classes-1]   (clothing only, masked by is_clothing)
        y_fabric    → class index in [0, num_fabric_classes-1]  (clothing only, masked)
        y_wash      → class index in [0, num_wash_classes-1]    (clothing only, masked)
        y_iscloth   → 0 (non-clothing) or 1 (clothing)          (all samples)
    """
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img = Image.open(row["image_path"]).convert("RGB")

        if self.transform:
            img = self.transform(img)

        y_color   = int(row["color_label_train"])
        y_fabric  = int(row["fabric_label_train"])
        y_wash    = int(row["wash_cycle_label_train"])
        y_iscloth = int(row["is_clothing"])

        return (
            img,
            torch.tensor(y_color,   dtype=torch.long),
            torch.tensor(y_fabric,  dtype=torch.long),
            torch.tensor(y_wash,    dtype=torch.long),
            torch.tensor(y_iscloth, dtype=torch.long),
        )


df_train = df_labels[df_labels["split"] == "train"].copy()
df_val   = df_labels[df_labels["split"] == "val"].copy()

print("Training rows  :", len(df_train))
print("Validation rows:", len(df_val))

train_dataset = WashMultiTaskDataset(df_train, train_transform)
val_dataset   = WashMultiTaskDataset(df_val,   val_transform)

BATCH_SIZE = 32

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=2,
    pin_memory=True,
)
val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True,
)

# -------------------------------------------------------------
# 2) Multi-Task ConvNeXt-Tiny Model (4 heads)
# -------------------------------------------------------------

class WashMultiTaskConvNeXt(nn.Module):
    def __init__(self, backbone_name="convnext_tiny",
                 num_color=4, num_fabric=5, num_wash=4, num_iscloth=2):
        super().__init__()

        self.backbone = timm.create_model(
            backbone_name,
            pretrained=True,
            num_classes=0,
            global_pool="avg",
        )

        feat_dim = self.backbone.num_features

        self.head_color      = nn.Linear(feat_dim, num_color)
        self.head_fabric     = nn.Linear(feat_dim, num_fabric)
        self.head_wash_cycle = nn.Linear(feat_dim, num_wash)
        self.head_is_cloth   = nn.Linear(feat_dim, num_iscloth)

    def forward(self, x):
        feat = self.backbone(x)
        return (
            self.head_color(feat),
            self.head_fabric(feat),
            self.head_wash_cycle(feat),
            self.head_is_cloth(feat),
        )


model = WashMultiTaskConvNeXt(
    backbone_name="convnext_tiny",
    num_color=num_color_classes,
    num_fabric=num_fabric_classes,
    num_wash=num_wash_classes,
    num_iscloth=num_iscloth_classes,
).to(device)

# -------------------------------------------------------------
# 3) Loss Functions, Optimizer, Scheduler
# -------------------------------------------------------------

criterion_color      = nn.CrossEntropyLoss()
criterion_fabric     = nn.CrossEntropyLoss()
criterion_wash_cycle = nn.CrossEntropyLoss()
criterion_iscloth    = nn.CrossEntropyLoss()

# Weighting the four tasks
w_color      = 0.35
w_fabric     = 0.25
w_wash_cycle = 0.25
w_iscloth    = 0.15

optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

NUM_EPOCHS = 16
best_val_wash_acc = 0.0
ckpt_path = os.path.join(OUTPUT_DIR, "best_model_wash.pt")

# -------------------------------------------------------------
# 4) Training & Validation Loop
# -------------------------------------------------------------

def run_epoch(loader, model, optimizer=None):
    """
    Runs one epoch of training or validation.
    If optimizer is None → evaluation mode.

    - is_clothing head uses all samples.
    - color/fabric/wash heads use only clothing samples (is_clothing == 1)
      via masking.
    """
    train_mode = optimizer is not None
    model.train(train_mode)

    total_loss = 0.0
    total_samples = 0

    correct_color  = 0
    correct_fabric = 0
    correct_wash   = 0
    correct_iscl   = 0

    total_color_samples  = 0
    total_fabric_samples = 0
    total_wash_samples   = 0

    for batch in tqdm(loader, leave=False):
        imgs, y_color, y_fabric, y_wash, y_iscloth = batch
        imgs       = imgs.to(device)
        y_color    = y_color.to(device)
        y_fabric   = y_fabric.to(device)
        y_wash     = y_wash.to(device)
        y_iscloth  = y_iscloth.to(device)

        if train_mode:
            optimizer.zero_grad()

        logits_color, logits_fabric, logits_wash, logits_iscloth = model(imgs)

        # Mask of clothing samples
        mask_cloth = (y_iscloth == 1)

        loss_color  = torch.tensor(0.0, device=device)
        loss_fabric = torch.tensor(0.0, device=device)
        loss_wash   = torch.tensor(0.0, device=device)

        if mask_cloth.any():
            loss_color  = criterion_color(logits_color[mask_cloth],  y_color[mask_cloth])
            loss_fabric = criterion_fabric(logits_fabric[mask_cloth], y_fabric[mask_cloth])
            loss_wash   = criterion_wash_cycle(logits_wash[mask_cloth], y_wash[mask_cloth])

        loss_iscl = criterion_iscloth(logits_iscloth, y_iscloth)

        loss = (
            w_color      * loss_color +
            w_fabric     * loss_fabric +
            w_wash_cycle * loss_wash +
            w_iscloth    * loss_iscl
        )

        if train_mode:
            loss.backward()
            optimizer.step()

        bs = imgs.size(0)
        total_loss    += loss.item() * bs
        total_samples += bs

        # Accuracies
        with torch.no_grad():
            # is_clothing on all samples
            pred_iscl = logits_iscloth.argmax(1)
            correct_iscl += (pred_iscl == y_iscloth).sum().item()

            # Other heads only on clothing
            if mask_cloth.any():
                pred_color  = logits_color.argmax(1)[mask_cloth]
                pred_fabric = logits_fabric.argmax(1)[mask_cloth]
                pred_wash   = logits_wash.argmax(1)[mask_cloth]

                yc = y_color[mask_cloth]
                yf = y_fabric[mask_cloth]
                yw = y_wash[mask_cloth]

                correct_color  += (pred_color  == yc).sum().item()
                correct_fabric += (pred_fabric == yf).sum().item()
                correct_wash   += (pred_wash   == yw).sum().item()

                total_color_samples  += yc.size(0)
                total_fabric_samples += yf.size(0)
                total_wash_samples   += yw.size(0)

    avg_loss = total_loss / max(total_samples, 1)

    color_acc  = correct_color  / max(total_color_samples, 1)
    fabric_acc = correct_fabric / max(total_fabric_samples, 1)
    wash_acc   = correct_wash   / max(total_wash_samples, 1)
    iscl_acc   = correct_iscl   / max(total_samples, 1)

    return avg_loss, color_acc, fabric_acc, wash_acc, iscl_acc


print("\n===============================")
print("Starting Multi-Task Training (4 heads)...")
print("===============================\n")

for epoch in range(1, NUM_EPOCHS + 1):
    start = time.time()

    train_loss, train_c_acc, train_f_acc, train_w_acc, train_iscl_acc = run_epoch(
        train_loader, model, optimizer
    )
    val_loss, val_c_acc, val_f_acc, val_w_acc, val_iscl_acc = run_epoch(
        val_loader, model, optimizer=None
    )

    scheduler.step()

    elapsed = (time.time() - start) / 60

    print(
        f"[Epoch {epoch:02d}] "
        f"TrainLoss={train_loss:.4f} | "
        f"TrainColor={train_c_acc*100:5.2f}% | "
        f"TrainFabric={train_f_acc*100:5.2f}% | "
        f"TrainWash={train_w_acc*100:5.2f}% | "
        f"TrainIsCl={train_iscl_acc*100:5.2f}% || "
        f"ValLoss={val_loss:.4f} | "
        f"ValColor={val_c_acc*100:5.2f}% | "
        f"ValFabric={val_f_acc*100:5.2f}% | "
        f"ValWash={val_w_acc*100:5.2f}% | "
        f"ValIsCl={val_iscl_acc*100:5.2f}% | "
        f"Time={elapsed:.1f} min"
    )

    # Use wash-cycle accuracy as the main checkpoint criterion (you can change this)
    if val_w_acc > best_val_wash_acc:
        best_val_wash_acc = val_w_acc
        torch.save(model.state_dict(), ckpt_path)
        print(f"  ✓ New best model saved (ValWashAcc={val_w_acc*100:.2f}%)")

print("\nTraining completed.")
print(f"Best validation wash-cycle accuracy: {best_val_wash_acc*100:.2f}%")
print("Best checkpoint saved at:", ckpt_path)

"""---

## 15) Evaluation of the Multi-Task Model

In this step, I evaluated the best saved multi-task ConvNeXt model on the validation set.  
I loaded the trained checkpoint, ran full inference on all validation images, and measured the accuracy of the four prediction heads: **color**, **fabric**, **wash-cycle**, and **is-clothing**.

For the first three heads, I evaluated performance **only on clothing samples**, while the is-clothing head was evaluated on **all** validation images.  
I also generated **row-normalized confusion matrices** to show how well each true class was separated from the others.

Finally, I displayed a set of **sample qualitative predictions**, comparing the model’s outputs with the true labels for random clothing items. This confirmed that the model performs consistently across different garment types and washing-cycle categories.

"""

# ======================================================================
# Cell 15 — Evaluation of the Best Multi-Task Model on the Validation Set
# ----------------------------------------------------------------------
# 4-head model:
#   • COLOR (clothing only)
#   • FABRIC (clothing only)
#   • WASH-CYCLE (clothing only)
#   • IS_CLOTHING (all images: clothing vs non-clothing)
# This cell:
#   • Loads the best saved checkpoint
#   • Evaluates accuracies on the VAL split
#   • Computes row-normalized confusion matrices for clothing-only heads
#   • Visualizes qualitative predictions for a few validation garments
#   • Confusion matrix for IS_CLOTHING (2×2)
# ======================================================================

import os
import itertools

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm.auto import tqdm
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import timm

# -----------------------------------------------------------
# 0) Paths, device setup, and label loading
# -----------------------------------------------------------
WASH_ROOT   = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV  = os.path.join(WASH_ROOT, "wash_labels.csv")
OUTPUT_DIR  = os.path.join(WASH_ROOT, "outputs_multitask_wash")
CKPT_PATH   = os.path.join(OUTPUT_DIR, "best_model_wash.pt")

assert os.path.exists(LABELS_CSV), f"Labels CSV not found: {LABELS_CSV}"
assert os.path.exists(CKPT_PATH),  f"Checkpoint not found: {CKPT_PATH}"

if timm is None:
    raise ImportError("timm is required for ConvNeXt evaluation. Please install timm first.")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

df_labels = pd.read_csv(LABELS_CSV)
print("Loaded labels:", len(df_labels))
print("Columns:", list(df_labels.columns))

assert "is_clothing" in df_labels.columns, "is_clothing column missing in labels CSV."
assert "split" in df_labels.columns, "split column missing in labels CSV."

# Keep VAL split (both clothing + non-clothing)
df_val_all = df_labels[df_labels["split"] == "val"].copy()
print("Validation rows (all):", len(df_val_all))
assert len(df_val_all) > 0, "No validation rows found."

# Clothing-only subset for color/fabric/wash evaluation
df_val_cloth = df_val_all[df_val_all["is_clothing"] == 1].copy()
print("Validation rows (clothing only):", len(df_val_cloth))

# Ensure label types are correct where defined
for col in ["color_label", "fabric_label", "wash_cycle_label"]:
    if col in df_val_all.columns:
        df_val_all[col] = df_val_all[col].fillna(0).astype(int)

df_val_all["is_clothing"] = df_val_all["is_clothing"].astype(int)

# Global class counts (on clothing subset) — consistent with Cell 15 (max + 1)
mask_cloth_global = (df_labels["is_clothing"] == 1)
num_color_classes  = int(df_labels.loc[mask_cloth_global, "color_label"].astype(int).max()) + 1
num_fabric_classes = int(df_labels.loc[mask_cloth_global, "fabric_label"].astype(int).max()) + 1
num_wash_classes   = int(df_labels.loc[mask_cloth_global, "wash_cycle_label"].astype(int).max()) + 1
num_iscloth_classes = 2  # {0: NON_CLOTHING, 1: CLOTHING}

print(f"COLOR classes      : {num_color_classes}")
print(f"FABRIC classes     : {num_fabric_classes}")
print(f"WASH-CYCLE classes : {num_wash_classes}")
print(f"IS_CLOTHING classes: {num_iscloth_classes}")

# Build human-readable class name mappings from CLOTHING validation data
df_val_cloth["color_label"]      = df_val_cloth["color_label"].astype(int)
df_val_cloth["fabric_label"]     = df_val_cloth["fabric_label"].astype(int)
df_val_cloth["wash_cycle_label"] = df_val_cloth["wash_cycle_label"].astype(int)

color_map  = df_val_cloth.groupby("color_label")["color_group"].agg(lambda x: x.mode()[0]).to_dict()
fabric_map = df_val_cloth.groupby("fabric_label")["fabric_group"].agg(lambda x: x.mode()[0]).to_dict()
wash_map   = df_val_cloth.groupby("wash_cycle_label")["wash_cycle"].agg(lambda x: x.mode()[0]).to_dict()

color_labels_sorted  = sorted(color_map.keys())
fabric_labels_sorted = sorted(fabric_map.keys())
wash_labels_sorted   = sorted(wash_map.keys())

color_names  = [color_map[i]  for i in color_labels_sorted]
fabric_names = [fabric_map[i] for i in fabric_labels_sorted]
wash_names   = [wash_map[i]   for i in wash_labels_sorted]

print("\nColor classes:",      list(zip(color_labels_sorted,  color_names)))
print("Fabric classes:",     list(zip(fabric_labels_sorted, fabric_names)))
print("Wash-cycle classes:", list(zip(wash_labels_sorted,   wash_names)))

# -----------------------------------------------------------
# 1) Dataset and preprocessing transforms
# -----------------------------------------------------------
IMG_SIZE = 256
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD  = [0.229, 0.224, 0.225]

val_transform = transforms.Compose([
    transforms.Resize(IMG_SIZE + 32),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
])


class WashValDataset(Dataset):
    """
    Validation dataset for the 4-head multitask model.
    Returns:
        image tensor,
        color_label,
        fabric_label,
        wash_cycle_label,
        is_clothing
    """
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img = Image.open(row["image_path"]).convert("RGB")
        if self.transform is not None:
            img = self.transform(img)

        # For non-clothing rows, these labels are dummy but harmless.
        y_color   = int(row.get("color_label", 0))
        y_fabric  = int(row.get("fabric_label", 0))
        y_wash    = int(row.get("wash_cycle_label", 0))
        y_iscloth = int(row["is_clothing"])

        return (
            img,
            torch.tensor(y_color,   dtype=torch.long),
            torch.tensor(y_fabric,  dtype=torch.long),
            torch.tensor(y_wash,    dtype=torch.long),
            torch.tensor(y_iscloth, dtype=torch.long),
        )


val_dataset = WashValDataset(df_val_all, transform=val_transform)
val_loader  = DataLoader(
    val_dataset,
    batch_size=32,
    shuffle=False,
    num_workers=0,
    pin_memory=True,
)

# -----------------------------------------------------------
# 2) 4-head Model definition (must match Cell 15)
# -----------------------------------------------------------
class WashMultiTaskConvNeXt(nn.Module):
    def __init__(self, backbone_name="convnext_tiny",
                 num_color=4, num_fabric=5, num_wash=4, num_iscloth=2):
        super().__init__()
        self.backbone = timm.create_model(
            backbone_name,
            pretrained=False,    # weights will be loaded from checkpoint
            num_classes=0,
            global_pool="avg",
        )
        feat_dim = self.backbone.num_features

        self.head_color      = nn.Linear(feat_dim, num_color)
        self.head_fabric     = nn.Linear(feat_dim, num_fabric)
        self.head_wash_cycle = nn.Linear(feat_dim, num_wash)
        self.head_is_cloth   = nn.Linear(feat_dim, num_iscloth)

    def forward(self, x):
        feat = self.backbone(x)
        return (
            self.head_color(feat),
            self.head_fabric(feat),
            self.head_wash_cycle(feat),
            self.head_is_cloth(feat),
        )


model = WashMultiTaskConvNeXt(
    backbone_name="convnext_tiny",
    num_color=num_color_classes,
    num_fabric=num_fabric_classes,
    num_wash=num_wash_classes,
    num_iscloth=num_iscloth_classes,
).to(device)

# Load best checkpoint from training
state_dict = torch.load(CKPT_PATH, map_location=device)
model.load_state_dict(state_dict)
model.eval()

print("\nLoaded best model checkpoint from:", CKPT_PATH)

# -----------------------------------------------------------
# 3) Inference on validation set
# -----------------------------------------------------------
all_y_color,  all_p_color  = [], []
all_y_fabric, all_p_fabric = [], []
all_y_wash,   all_p_wash   = [], []
all_y_iscl,   all_p_iscl   = [], []

with torch.no_grad():
    for batch in tqdm(val_loader, desc="Evaluating"):
        imgs, y_color, y_fabric, y_wash, y_iscloth = batch

        imgs      = imgs.to(device)
        y_color   = y_color.to(device)
        y_fabric  = y_fabric.to(device)
        y_wash    = y_wash.to(device)
        y_iscloth = y_iscloth.to(device)

        logits_color, logits_fabric, logits_wash, logits_iscloth = model(imgs)

        preds_color   = logits_color.argmax(dim=1)
        preds_fabric  = logits_fabric.argmax(dim=1)
        preds_wash    = logits_wash.argmax(dim=1)
        preds_iscloth = logits_iscloth.argmax(dim=1)

        all_y_color.extend(y_color.cpu().numpy())
        all_p_color.extend(preds_color.cpu().numpy())

        all_y_fabric.extend(y_fabric.cpu().numpy())
        all_p_fabric.extend(preds_fabric.cpu().numpy())

        all_y_wash.extend(y_wash.cpu().numpy())
        all_p_wash.extend(preds_wash.cpu().numpy())

        all_y_iscl.extend(y_iscloth.cpu().numpy())
        all_p_iscl.extend(preds_iscloth.cpu().numpy())

all_y_color  = np.array(all_y_color)
all_p_color  = np.array(all_p_color)
all_y_fabric = np.array(all_y_fabric)
all_p_fabric = np.array(all_p_fabric)
all_y_wash   = np.array(all_y_wash)
all_p_wash   = np.array(all_p_wash)
all_y_iscl   = np.array(all_y_iscl)
all_p_iscl   = np.array(all_p_iscl)

# Clothing-only masks for the first 3 heads
mask_val_cloth = (all_y_iscl == 1)

y_color_cloth  = all_y_color[mask_val_cloth]
p_color_cloth  = all_p_color[mask_val_cloth]
y_fabric_cloth = all_y_fabric[mask_val_cloth]
p_fabric_cloth = all_p_fabric[mask_val_cloth]
y_wash_cloth   = all_y_wash[mask_val_cloth]
p_wash_cloth   = all_p_wash[mask_val_cloth]

# Overall accuracies
color_acc   = (y_color_cloth  == p_color_cloth).mean()
fabric_acc  = (y_fabric_cloth == p_fabric_cloth).mean()
wash_acc    = (y_wash_cloth   == p_wash_cloth).mean()
iscloth_acc = (all_y_iscl     == all_p_iscl).mean()

print("\n================ VALIDATION ACCURACY ================")
print(f"Color accuracy (clothing only)      : {color_acc   * 100:6.2f}%")
print(f"Fabric accuracy (clothing only)     : {fabric_acc  * 100:6.2f}%")
print(f"Wash-cycle accuracy (clothing only) : {wash_acc    * 100:6.2f}%")
print(f"Is-clothing accuracy (all VAL rows) : {iscloth_acc * 100:6.2f}%")

# -----------------------------------------------------------
# 4) Confusion matrix plotting (row-normalized, clothing only)
# -----------------------------------------------------------
def plot_confusion_matrix(cm, classes, title, normalize=True):
    """
    Plot a confusion matrix, optionally row-normalized (% per true class).
    """
    if normalize:
        cm = cm.astype(float)
        row_sums = cm.sum(axis=1, keepdims=True)
        cm = np.divide(cm, row_sums, out=np.zeros_like(cm, dtype=float), where=row_sums != 0)

    plt.figure(figsize=(7, 6))
    im = plt.imshow(cm, interpolation="nearest", cmap="Blues")
    plt.title(title, fontsize=12)
    plt.colorbar(im, fraction=0.046, pad=0.04)

    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45, ha="right", fontsize=9)
    plt.yticks(tick_marks, classes, fontsize=9)

    thresh = cm.max() / 2.0
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            txt = f"{cm[i, j] * 100:.1f}%"
        else:
            txt = str(int(cm[i, j]))
        plt.text(
            j, i, txt,
            ha="center",
            va="center",
            color="white" if cm[i, j] > thresh else "black",
            fontsize=8,
        )

    plt.ylabel("True label", fontsize=10)
    plt.xlabel("Predicted label", fontsize=10)
    plt.tight_layout()
    plt.show()


# Confusion matrices for CLOTHING ONLY (3 heads)
cm_color  = confusion_matrix(y_color_cloth,  p_color_cloth,  labels=color_labels_sorted)
cm_fabric = confusion_matrix(y_fabric_cloth, p_fabric_cloth, labels=fabric_labels_sorted)
cm_wash   = confusion_matrix(y_wash_cloth,   p_wash_cloth,   labels=wash_labels_sorted)

plot_confusion_matrix(
    cm_color,
    color_names,
    "Validation Confusion Matrix — COLOR (row-normalized, clothing only)",
)

plot_confusion_matrix(
    cm_fabric,
    fabric_names,
    "Validation Confusion Matrix — FABRIC (row-normalized, clothing only)",
)

plot_confusion_matrix(
    cm_wash,
    wash_names,
    "Validation Confusion Matrix — WASH-CYCLE (row-normalized, clothing only)",
)

# Confusion matrix for IS_CLOTHING (2×2, all VAL rows)
cm_iscl = confusion_matrix(all_y_iscl, all_p_iscl, labels=[0, 1])
plot_confusion_matrix(
    cm_iscl,
    ["NON_CLOTHING", "CLOTHING"],
    "Validation Confusion Matrix — IS_CLOTHING (row-normalized, all VAL rows)",
)

# -----------------------------------------------------------
# 5) Qualitative predictions on random clothing validation samples
# -----------------------------------------------------------
idx2color  = {k: v for k, v in color_map.items()}
idx2fabric = {k: v for k, v in fabric_map.items()}
idx2wash   = {k: v for k, v in wash_map.items()}


def show_random_examples(n=8, seed=42):
    """
    Show n random CLOTHING validation images with true vs predicted
    COLOR / FABRIC / WASH-CYCLE labels.
    """
    subset = df_val_cloth.sample(min(n, len(df_val_cloth)), random_state=seed).reset_index(drop=True)
    rows = (len(subset) + 1) // 2

    plt.figure(figsize=(10, 3 * rows))

    for i, row in subset.iterrows():
        img_path = row["image_path"]
        img = Image.open(img_path).convert("RGB")

        x = val_transform(img).unsqueeze(0).to(device)

        with torch.no_grad():
            logits_color, logits_fabric, logits_wash, logits_iscloth = model(x)
            pc = logits_color.argmax(dim=1).item()
            pf = logits_fabric.argmax(dim=1).item()
            pw = logits_wash.argmax(dim=1).item()

        plt.subplot(rows, 2, i + 1)
        plt.imshow(img)
        plt.axis("off")
        plt.title(
            f"TRUE: {idx2color[row['color_label']]} | {idx2fabric[row['fabric_label']]}\n"
            f"      {idx2wash[row['wash_cycle_label']]}\n"
            f"PRED: {idx2color.get(pc, '?')} | {idx2fabric.get(pf, '?')}\n"
            f"      {idx2wash.get(pw, '?')}",
            fontsize=9,
        )

    plt.tight_layout()
    plt.show()


print("\nShowing sample predictions from the (clothing-only) validation set:")
show_random_examples(n=8, seed=0)

"""---

### 16) Coherent Summary of the Full Evaluation

In this stage, I performed the complete evaluation of my trained four-head ConvNeXt model on the validation split. I first loaded the best checkpoint and generated predictions for all images. I then computed all final metrics, including color, fabric, wash-cycle, and clothing-detection accuracy, and summarized them in CSV reports. I also built row-normalized confusion matrices and visualized them as heatmaps, alongside per-class accuracy bar plots for COLOR, FABRIC, and WASH_CYCLE. Finally, I showed coherent qualitative examples by displaying only those clothing images for which all three garment-related predictions were correct.

"""

# =====================================================================
# Cell 16 — Full Evaluation & Reporting for the 4-Head Multi-Task Model
# ---------------------------------------------------------------------
# Model heads:
#   • COLOR (clothing only)
#   • FABRIC (clothing only)
#   • WASH_CYCLE (clothing only)
#   • IS_CLOTHING (all images: clothing vs non-clothing)
#
# This cell:
#   • Loads the best saved checkpoint
#   • Evaluates all 4 heads on the validation split
#   • Computes and saves:
#       - Row-normalized confusion matrices (PNG files)
#       - Per-class accuracy bar charts for COLOR/FABRIC/WASH_CYCLE
#   • Exports numeric metrics (overall + per-class) to CSV
#   • Visualizes random qualitative examples (only correct predictions)
# =====================================================================

import os
import itertools

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm.auto import tqdm
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import timm

# ---------------------------------------------------------------------
# 0) Paths, device selection, and label loading
# ---------------------------------------------------------------------
WASH_ROOT   = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV  = os.path.join(WASH_ROOT, "wash_labels.csv")
OUTPUT_DIR  = os.path.join(WASH_ROOT, "outputs_multitask_wash")
CKPT_PATH   = os.path.join(OUTPUT_DIR, "best_model_wash.pt")

assert os.path.exists(LABELS_CSV), f"Labels CSV not found: {LABELS_CSV}"
assert os.path.exists(CKPT_PATH),  f"Checkpoint not found: {CKPT_PATH}"

os.makedirs(OUTPUT_DIR, exist_ok=True)

if timm is None:
    raise ImportError("timm is required for ConvNeXt evaluation. Please install timm.")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

df_labels = pd.read_csv(LABELS_CSV)
print("Loaded labels:", len(df_labels))
print("Columns:", list(df_labels.columns))

assert "split"       in df_labels.columns, "'split' column is missing."
assert "is_clothing" in df_labels.columns, "'is_clothing' column is missing."

# Validation split (clothing + non-clothing)
df_val_all = df_labels[df_labels["split"] == "val"].copy()
print("Validation rows (all):", len(df_val_all))
assert len(df_val_all) > 0, "No validation rows in the VAL split."

# Clothing-only subset for garment heads
df_val_cloth = df_val_all[df_val_all["is_clothing"] == 1].copy()
print("Validation rows (clothing only):", len(df_val_cloth))

# Ensure integer dtypes
for col in ["color_label", "fabric_label", "wash_cycle_label", "is_clothing"]:
    if col in df_val_all.columns:
        df_val_all[col] = df_val_all[col].fillna(0).astype(int)

# Global number of classes on the full CSV (restricted to clothing rows where needed)
mask_cloth_global = (df_labels["is_clothing"] == 1)

num_color_classes  = int(df_labels.loc[mask_cloth_global, "color_label"].astype(int).max()) + 1
num_fabric_classes = int(df_labels.loc[mask_cloth_global, "fabric_label"].astype(int).max()) + 1
num_wash_classes   = int(df_labels.loc[mask_cloth_global, "wash_cycle_label"].astype(int).max()) + 1
num_iscloth_classes = 2  # 0 = NON_CLOTHING, 1 = CLOTHING

print(f"GLOBAL COLOR classes      : {num_color_classes}")
print(f"GLOBAL FABRIC classes     : {num_fabric_classes}")
print(f"GLOBAL WASH_CYCLE classes : {num_wash_classes}")
print(f"GLOBAL IS_CLOTHING classes: {num_iscloth_classes}")

# Build index → name mappings from CLOTHING VAL set
df_val_cloth["color_label"]      = df_val_cloth["color_label"].astype(int)
df_val_cloth["fabric_label"]     = df_val_cloth["fabric_label"].astype(int)
df_val_cloth["wash_cycle_label"] = df_val_cloth["wash_cycle_label"].astype(int)

color_map  = df_val_cloth.groupby("color_label")     ["color_group"].agg(lambda x: x.mode()[0]).to_dict()
fabric_map = df_val_cloth.groupby("fabric_label")    ["fabric_group"].agg(lambda x: x.mode()[0]).to_dict()
wash_map   = df_val_cloth.groupby("wash_cycle_label")["wash_cycle"].agg(lambda x: x.mode()[0]).to_dict()

color_indices  = sorted(color_map.keys())
fabric_indices = sorted(fabric_map.keys())
wash_indices   = sorted(wash_map.keys())

color_names  = [color_map[i]  for i in color_indices]
fabric_names = [fabric_map[i] for i in fabric_indices]
wash_names   = [wash_map[i]   for i in wash_indices]

print("\nColor classes:     ", list(zip(color_indices,  color_names)))
print("Fabric classes:    ", list(zip(fabric_indices, fabric_names)))
print("Wash-cycle classes:", list(zip(wash_indices,   wash_names)))

# ---------------------------------------------------------------------
# 1) Dataset and transforms for the validation split
# ---------------------------------------------------------------------
IMG_SIZE = 256
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD  = [0.229, 0.224, 0.225]

val_transform = transforms.Compose([
    transforms.Resize(IMG_SIZE + 32),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
])


class WashValDataset(Dataset):
    """
    Validation dataset for the 4-head multitask model.

    Returns:
        image_tensor,
        color_label,
        fabric_label,
        wash_cycle_label,
        is_clothing
    """
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = row["image_path"]

        img = Image.open(img_path).convert("RGB")
        if self.transform is not None:
            img = self.transform(img)

        y_color      = int(row.get("color_label", 0))
        y_fabric     = int(row.get("fabric_label", 0))
        y_wash_cycle = int(row.get("wash_cycle_label", 0))
        y_iscloth    = int(row["is_clothing"])

        return (
            img,
            torch.tensor(y_color,      dtype=torch.long),
            torch.tensor(y_fabric,     dtype=torch.long),
            torch.tensor(y_wash_cycle, dtype=torch.long),
            torch.tensor(y_iscloth,    dtype=torch.long),
        )


val_dataset = WashValDataset(df_val_all, transform=val_transform)
val_loader  = DataLoader(
    val_dataset,
    batch_size=32,
    shuffle=False,
    num_workers=0,
    pin_memory=True,
)

# ---------------------------------------------------------------------
# 2) 4-head model definition (must match training)
# ---------------------------------------------------------------------
BACKBONE_NAME = "convnext_tiny"


class WashMultiTaskConvNeXt(nn.Module):
    """
    ConvNeXt backbone with four classification heads:
    - COLOR
    - FABRIC
    - WASH_CYCLE
    - IS_CLOTHING
    """
    def __init__(self,
                 backbone_name=BACKBONE_NAME,
                 num_color=4,
                 num_fabric=5,
                 num_wash=4,
                 num_iscloth=2):
        super().__init__()
        self.backbone = timm.create_model(
            backbone_name,
            pretrained=False,   # weights loaded from checkpoint
            num_classes=0,
            global_pool="avg",
        )
        feat_dim = self.backbone.num_features

        self.head_color      = nn.Linear(feat_dim, num_color)
        self.head_fabric     = nn.Linear(feat_dim, num_fabric)
        self.head_wash_cycle = nn.Linear(feat_dim, num_wash)
        self.head_is_cloth   = nn.Linear(feat_dim, num_iscloth)

    def forward(self, x):
        feat = self.backbone(x)
        logits_color      = self.head_color(feat)
        logits_fabric     = self.head_fabric(feat)
        logits_wash_cycle = self.head_wash_cycle(feat)
        logits_is_cloth   = self.head_is_cloth(feat)
        return logits_color, logits_fabric, logits_wash_cycle, logits_is_cloth


model = WashMultiTaskConvNeXt(
    backbone_name=BACKBONE_NAME,
    num_color=num_color_classes,
    num_fabric=num_fabric_classes,
    num_wash=num_wash_classes,
    num_iscloth=num_iscloth_classes,
).to(device)

# Load the best checkpoint
state_dict = torch.load(CKPT_PATH, map_location=device)
model.load_state_dict(state_dict)
model.eval()

print("\nLoaded best model from:", CKPT_PATH)

# ---------------------------------------------------------------------
# 3) Run inference on VAL and collect predictions
# ---------------------------------------------------------------------
all_y_color,  all_p_color  = [], []
all_y_fabric, all_p_fabric = [], []
all_y_wash,   all_p_wash   = [], []
all_y_iscl,   all_p_iscl   = [], []

with torch.no_grad():
    for batch in tqdm(val_loader, desc="Evaluating on VAL"):
        imgs, y_color, y_fabric, y_wash, y_iscloth = batch

        imgs      = imgs.to(device, non_blocking=True)
        y_color   = y_color.to(device)
        y_fabric  = y_fabric.to(device)
        y_wash    = y_wash.to(device)
        y_iscloth = y_iscloth.to(device)

        logits_color, logits_fabric, logits_wash, logits_iscloth = model(imgs)

        preds_color   = logits_color.argmax(dim=1)
        preds_fabric  = logits_fabric.argmax(dim=1)
        preds_wash    = logits_wash.argmax(dim=1)
        preds_iscloth = logits_iscloth.argmax(dim=1)

        all_y_color.extend(y_color.cpu().numpy())
        all_p_color.extend(preds_color.cpu().numpy())

        all_y_fabric.extend(y_fabric.cpu().numpy())
        all_p_fabric.extend(preds_fabric.cpu().numpy())

        all_y_wash.extend(y_wash.cpu().numpy())
        all_p_wash.extend(preds_wash.cpu().numpy())

        all_y_iscl.extend(y_iscloth.cpu().numpy())
        all_p_iscl.extend(preds_iscloth.cpu().numpy())

all_y_color  = np.array(all_y_color)
all_p_color  = np.array(all_p_color)
all_y_fabric = np.array(all_y_fabric)
all_p_fabric = np.array(all_p_fabric)
all_y_wash   = np.array(all_y_wash)
all_p_wash   = np.array(all_p_wash)
all_y_iscl   = np.array(all_y_iscl)
all_p_iscl   = np.array(all_p_iscl)

# Attach predictions to df_val_all (same ordering: shuffle=False + reset_index)
df_val_all = df_val_all.reset_index(drop=True)
df_val_all["pred_color_label"]      = all_p_color
df_val_all["pred_fabric_label"]     = all_p_fabric
df_val_all["pred_wash_cycle_label"] = all_p_wash
df_val_all["pred_is_clothing"]      = all_p_iscl

# Clothing-only arrays for garment heads
mask_val_cloth = (all_y_iscl == 1)

y_color_cloth  = all_y_color[mask_val_cloth]
p_color_cloth  = all_p_color[mask_val_cloth]
y_fabric_cloth = all_y_fabric[mask_val_cloth]
p_fabric_cloth = all_p_fabric[mask_val_cloth]
y_wash_cloth   = all_y_wash[mask_val_cloth]
p_wash_cloth   = all_p_wash[mask_val_cloth]

# ---------------------------------------------------------------------
# 4) Overall accuracies and numeric summary
# ---------------------------------------------------------------------
color_accuracy   = (y_color_cloth  == p_color_cloth).mean()
fabric_accuracy  = (y_fabric_cloth == p_fabric_cloth).mean()
wash_accuracy    = (y_wash_cloth   == p_wash_cloth).mean()
iscloth_accuracy = (all_y_iscl     == all_p_iscl).mean()

print("\n=== VALIDATION ACCURACIES ===")
print(f"Color accuracy (clothing only)      : {color_accuracy   * 100:6.2f}%")
print(f"Fabric accuracy (clothing only)     : {fabric_accuracy  * 100:6.2f}%")
print(f"Wash-cycle accuracy (clothing only) : {wash_accuracy    * 100:6.2f}%")
print(f"Is-clothing accuracy (all VAL rows) : {iscloth_accuracy * 100:6.2f}%")

metrics_df = pd.DataFrame({
    "metric": [
        "color_accuracy_clothing",
        "fabric_accuracy_clothing",
        "wash_cycle_accuracy_clothing",
        "is_clothing_accuracy_all",
    ],
    "value":  [
        color_accuracy,
        fabric_accuracy,
        wash_accuracy,
        iscloth_accuracy,
    ],
})
metrics_path = os.path.join(OUTPUT_DIR, "val_overall_metrics_4head.csv")
metrics_df.to_csv(metrics_path, index=False)
print("\nSaved overall validation metrics to:", metrics_path)

# ---------------------------------------------------------------------
# 5) Confusion matrix plotting helper (row-normalized)
# ---------------------------------------------------------------------
def plot_confusion_matrix(cm, classes, title, filename, normalize=True):
    """
    Plot a confusion matrix, optionally row-normalized (per true class).
    """
    if normalize:
        cm = cm.astype(float)
        row_sums = cm.sum(axis=1, keepdims=True)
        cm = np.divide(cm, row_sums, out=np.zeros_like(cm), where=row_sums != 0)

    fig, ax = plt.subplots(figsize=(6, 5))
    im = ax.imshow(cm, interpolation="nearest", cmap="Blues")
    ax.set_title(title)

    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.ax.set_ylabel("Proportion", rotation=-90, va="bottom")

    tick_marks = np.arange(len(classes))
    ax.set_xticks(tick_marks)
    ax.set_xticklabels(classes, rotation=45, ha="right")
    ax.set_yticks(tick_marks)
    ax.set_yticklabels(classes)

    thresh = cm.max() / 2.0
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            text_value = 100.0 * cm[i, j]
            s = f"{text_value:.1f}%"
        else:
            s = str(int(cm[i, j]))
        ax.text(
            j, i, s,
            ha="center", va="center",
            color="white" if cm[i, j] > thresh else "black",
            fontsize=9,
        )

    ax.set_ylabel("True label")
    ax.set_xlabel("Predicted label")
    fig.tight_layout()

    save_path = os.path.join(OUTPUT_DIR, filename)
    fig.savefig(save_path, dpi=200, bbox_inches="tight")
    print("Saved figure:", save_path)

    plt.show()


# Confusion matrices (clothing only for garment heads)
cm_color  = confusion_matrix(y_color_cloth,  p_color_cloth,  labels=color_indices)
cm_fabric = confusion_matrix(y_fabric_cloth, p_fabric_cloth, labels=fabric_indices)
cm_wash   = confusion_matrix(y_wash_cloth,   p_wash_cloth,   labels=wash_indices)

plot_confusion_matrix(
    cm_color,
    color_names,
    "VAL Confusion Matrix — COLOR (row-normalized, clothing only)",
    "cm_color_val_4head.png",
)

plot_confusion_matrix(
    cm_fabric,
    fabric_names,
    "VAL Confusion Matrix — FABRIC (row-normalized, clothing only)",
    "cm_fabric_val_4head.png",
)

plot_confusion_matrix(
    cm_wash,
    wash_names,
    "VAL Confusion Matrix — WASH_CYCLE (row-normalized, clothing only)",
    "cm_wash_val_4head.png",
)

# Simple 2x2 confusion matrix for is_clothing (all rows)
cm_iscloth = confusion_matrix(all_y_iscl, all_p_iscl, labels=[0, 1])
plot_confusion_matrix(
    cm_iscloth,
    ["NON_CLOTHING", "CLOTHING"],
    "VAL Confusion Matrix — IS_CLOTHING (row-normalized, all VAL)",
    "cm_is_clothing_val_4head.png",
)

# ---------------------------------------------------------------------
# 6) Per-class accuracies and bar plots (COLOR/FABRIC/WASH_CYCLE)
# ---------------------------------------------------------------------
def per_class_accuracy(all_true, all_pred, indices, names, task_name):
    """
    Compute per-class accuracy for a given task.
    """
    rows = []
    for idx, name in zip(indices, names):
        mask = (all_true == idx)
        support = mask.sum()
        if support == 0:
            acc = np.nan
        else:
            acc = (all_pred[mask] == all_true[mask]).mean()
        rows.append({
            "task":        task_name,
            "class_index": idx,
            "class_name":  name,
            "support":     int(support),
            "accuracy":    float(acc),
        })
    return pd.DataFrame(rows)


df_color_acc  = per_class_accuracy(
    y_color_cloth,  p_color_cloth,  color_indices,  color_names,  "COLOR"
)
df_fabric_acc = per_class_accuracy(
    y_fabric_cloth, p_fabric_cloth, fabric_indices, fabric_names, "FABRIC"
)
df_wash_acc   = per_class_accuracy(
    y_wash_cloth,   p_wash_cloth,   wash_indices,   wash_names,   "WASH_CYCLE"
)

per_class_df = pd.concat([df_color_acc, df_fabric_acc, df_wash_acc], ignore_index=True)
per_class_path = os.path.join(OUTPUT_DIR, "val_per_class_accuracies_4head.csv")
per_class_df.to_csv(per_class_path, index=False)
print("\nSaved per-class accuracies to:", per_class_path)


def plot_per_class_bar(df_task, title, filename):
    """
    Per-class accuracy bar chart:
      • Classes sorted by accuracy (high → low)
      • Distinct colors per bar
    """
    df_plot = df_task.dropna(subset=["accuracy"]).copy()
    df_plot = df_plot.sort_values(["accuracy", "class_name"],
                                  ascending=[False, True]).reset_index(drop=True)

    names      = df_plot["class_name"].tolist()
    accuracies = df_plot["accuracy"].tolist()

    x = np.arange(len(names))

    cmap = plt.cm.get_cmap("tab20", len(names))
    colors = [cmap(i) for i in range(len(names))]

    fig, ax = plt.subplots(figsize=(8, 4.5))
    ax.bar(x, accuracies, color=colors)

    ax.set_xticks(x)
    ax.set_xticklabels(names, rotation=45, ha="right")
    ax.set_ylabel("Accuracy")
    ax.set_ylim(0.0, 1.0)
    ax.set_title(title)

    for xi, acc in zip(x, accuracies):
        text = f"{acc * 100:.1f}%"
        y = min(acc + 0.03, 1.02)
        ax.text(xi, y, text, ha="center", va="bottom", fontsize=9)

    fig.tight_layout()
    save_path = os.path.join(OUTPUT_DIR, filename)
    fig.savefig(save_path, dpi=200, bbox_inches="tight")
    print("Saved figure:", save_path)
    plt.show()


plot_per_class_bar(
    df_color_acc,
    "VAL Per-Class Accuracy — COLOR (clothing only)",
    "per_class_accuracy_color_4head.png",
)

plot_per_class_bar(
    df_fabric_acc,
    "VAL Per-Class Accuracy — FABRIC (clothing only)",
    "per_class_accuracy_fabric_4head.png",
)

plot_per_class_bar(
    df_wash_acc,
    "VAL Per-Class Accuracy — WASH_CYCLE (clothing only)",
    "per_class_accuracy_wash_4head.png",
)

# ---------------------------------------------------------------------
# 7) Qualitative examples: show only correct predictions (all 3 heads)
# ---------------------------------------------------------------------
idx2color  = {k: v for k, v in color_map.items()}
idx2fabric = {k: v for k, v in fabric_map.items()}
idx2wash   = {k: v for k, v in wash_map.items()}

# Only clothing rows where COLOR/FABRIC/WASH are all correct
df_val_cloth_full = df_val_all[df_val_all["is_clothing"] == 1].copy()
mask_correct = (
    (df_val_cloth_full["color_label"]      == df_val_cloth_full["pred_color_label"]) &
    (df_val_cloth_full["fabric_label"]     == df_val_cloth_full["pred_fabric_label"]) &
    (df_val_cloth_full["wash_cycle_label"] == df_val_cloth_full["pred_wash_cycle_label"])
)
df_correct = df_val_cloth_full[mask_correct].reset_index(drop=True)
print("\nCorrectly predicted clothing samples:", len(df_correct))


def show_random_examples(n=8, seed=42):
    """
    Show n clothing validation images for which
    all three heads (COLOR/FABRIC/WASH) are correct.
    """
    if len(df_correct) == 0:
        print("No fully-correct clothing examples available to display.")
        return

    subset = df_correct.sample(min(n, len(df_correct)), random_state=seed).reset_index(drop=True)
    rows = int(np.ceil(len(subset) / 2))

    fig, axes = plt.subplots(rows, 2, figsize=(8, 4 * rows))
    axes = np.array(axes).reshape(-1)

    for i, row in enumerate(subset.itertuples(), start=0):
        img_path = row.image_path

        img_pil = Image.open(img_path).convert("RGB")

        true_c = idx2color[row.color_label]
        true_f = idx2fabric[row.fabric_label]
        true_w = idx2wash[row.wash_cycle_label]

        ax = axes[i]
        ax.imshow(img_pil)
        ax.axis("off")
        ax.set_title(
            f"TRUE = PRED:\n"
            f"{true_c} | {true_f}\n"
            f"{true_w}",
            fontsize=9,
        )

    for j in range(len(subset), len(axes)):
        axes[j].axis("off")

    fig.tight_layout()
    examples_path = os.path.join(OUTPUT_DIR, "val_example_predictions_correct_4head.png")
    fig.savefig(examples_path, dpi=200, bbox_inches="tight")
    print("\nSaved example predictions figure to:", examples_path)
    plt.show()


print("\nShowing sample clothing validation images with all heads correct:")
show_random_examples(n=8, seed=0)

"""---

### 17) loaded the final `wash_labels.csv
In this step, I loaded the final `wash_labels.csv` and extracted all unique label–name mappings required for the four-head model. I organized the mappings for **color**, **fabric**, **wash-cycle**, and **is_clothing**, cleaned them, sorted them, and removed duplicates. After preparing the tables, I exported each mapping as a separate CSV file inside the output directory, ensuring they could be reused later for reporting, demos, or deployment.

"""

# ============================================================
# Cell 17 — Export Label Mappings (4-Head Model)
# ------------------------------------------------------------
# This cell:
#   • Loads the final wash_labels.csv
#   • Extracts unique mappings:
#         color_label       → color_group
#         fabric_label      → fabric_group
#         wash_cycle_label  → wash_cycle
#         is_clothing       → {NON_CLOTHING, CLOTHING}  (4th head)
#   • Saves each mapping as a separate CSV under OUTPUT_DIR
#     so they can be reused in reports, demos, or deployment.
# ============================================================

import os
import pandas as pd

# -----------------------------
# Core paths
# -----------------------------
WASH_ROOT   = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV  = os.path.join(WASH_ROOT, "wash_labels.csv")
OUTPUT_DIR  = os.path.join(WASH_ROOT, "outputs_multitask_wash")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# -----------------------------
# Load full label table
# -----------------------------
df = pd.read_csv(LABELS_CSV)
print("Loaded labels:", len(df))
print("Columns:", list(df.columns))

required_cols = [
    "color_label", "color_group",
    "fabric_label", "fabric_group",
    "wash_cycle_label", "wash_cycle",
    "is_clothing",
]
for c in required_cols:
    if c not in df.columns:
        raise KeyError(f"Required column '{c}' is missing from wash_labels.csv")

# Make sure is_clothing is integer-typed
df["is_clothing"] = df["is_clothing"].fillna(0).astype(int)

# ----------------------------------------------------------------
# 1) Extract unique (index → name) mappings for each garment head
#     (convert to int only AFTER dropping NaNs)
# ----------------------------------------------------------------
color_map_df = (
    df[["color_label", "color_group"]]
    .dropna(subset=["color_label", "color_group"])
    .assign(color_label=lambda d: d["color_label"].astype(int))
    .drop_duplicates()
    .sort_values("color_label")
    .reset_index(drop=True)
)

fabric_map_df = (
    df[["fabric_label", "fabric_group"]]
    .dropna(subset=["fabric_label", "fabric_group"])
    .assign(fabric_label=lambda d: d["fabric_label"].astype(int))
    .drop_duplicates()
    .sort_values("fabric_label")
    .reset_index(drop=True)
)

wash_map_df = (
    df[["wash_cycle_label", "wash_cycle"]]
    .dropna(subset=["wash_cycle_label", "wash_cycle"])
    .assign(wash_cycle_label=lambda d: d["wash_cycle_label"].astype(int))
    .drop_duplicates()
    .sort_values("wash_cycle_label")
    .reset_index(drop=True)
)

print("\nCOLOR mapping:")
print(color_map_df)

print("\nFABRIC mapping:")
print(fabric_map_df)

print("\nWASH_CYCLE mapping:")
print(wash_map_df)

# ----------------------------------------------------------------
# 2) 4th head: IS_CLOTHING mapping (0/1 → class_name)
# ----------------------------------------------------------------
name_map = {0: "NON_CLOTHING", 1: "CLOTHING"}

iscloth_unique = sorted(int(v) for v in pd.unique(df["is_clothing"].dropna()))
iscloth_rows = []
for v in iscloth_unique:
    iscloth_rows.append({
        "is_clothing": int(v),
        "class_name":  name_map.get(int(v), f"CLASS_{int(v)}"),
    })

iscloth_map_df = (
    pd.DataFrame(iscloth_rows)
    .sort_values("is_clothing")
    .reset_index(drop=True)
)

print("\nIS_CLOTHING mapping:")
print(iscloth_map_df)

# ----------------------------------------------------------------
# 3) Save mappings to disk
# ----------------------------------------------------------------
color_map_path    = os.path.join(OUTPUT_DIR, "mapping_color_label.csv")
fabric_map_path   = os.path.join(OUTPUT_DIR, "mapping_fabric_label.csv")
wash_map_path     = os.path.join(OUTPUT_DIR, "mapping_wash_cycle_label.csv")
iscloth_map_path  = os.path.join(OUTPUT_DIR, "mapping_is_clothing.csv")

color_map_df.to_csv(color_map_path,   index=False)
fabric_map_df.to_csv(fabric_map_path, index=False)
wash_map_df.to_csv(wash_map_path,     index=False)
iscloth_map_df.to_csv(iscloth_map_path, index=False)

print("\nSaved label mappings:")
print("  • COLOR        →", color_map_path)
print("  • FABRIC       →", fabric_map_path)
print("  • WASH_CYCLE   →", wash_map_path)
print("  • IS_CLOTHING  →", iscloth_map_path)

"""---

### 18)  loaded the trained **4-head ConvNeXt model Demo

In this step, I loaded the trained **4-head ConvNeXt model** and ran batch inference on **clothing-only** samples. I applied the same preprocessing used in validation, performed forward passes through all four heads, and collected both **true** and **predicted** labels for color, fabric, wash-cycle, and is_clothing. After assembling all predictions, I exported them into a clean `demo_predictions.csv` file for later use in the demo interface and reporting.

"""

# ============================================================
# Cell 18 — Batch Inference for DEMO (demo_predictions.csv)
# ------------------------------------------------------------
# 4-head multitask model:
#   • color_label
#   • fabric_label
#   • wash_cycle_label
#   • is_clothing
#
# This cell:
#   • Loads the best multitask 4-head model checkpoint
#   • Runs batch inference over a chosen evaluation subset
#   • Saves a CSV with true vs predicted labels for:
#         - color_label
#         - fabric_label
#         - wash_cycle_label
#         - is_clothing
#   • For the DEMO CSV, we restrict to CLOTHING rows
#     (is_clothing == 1), because wash_cycle is only defined
#     for garments.
# ============================================================

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm.auto import tqdm
import timm

# -------------------------------------------------------------------
# 0) Paths, device, and label table
# -------------------------------------------------------------------
WASH_ROOT   = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV  = os.path.join(WASH_ROOT, "wash_labels.csv")
OUTPUT_DIR  = os.path.join(WASH_ROOT, "outputs_multitask_wash")
CKPT_PATH   = os.path.join(OUTPUT_DIR, "best_model_wash.pt")

assert os.path.exists(LABELS_CSV), f"Labels CSV not found: {LABELS_CSV}"
assert os.path.exists(CKPT_PATH),  f"Checkpoint not found: {CKPT_PATH}"
os.makedirs(OUTPUT_DIR, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

df_all = pd.read_csv(LABELS_CSV)
print("Total rows in wash_labels.csv:", len(df_all))
print("Columns:", list(df_all.columns))

assert "is_clothing" in df_all.columns, "Column 'is_clothing' missing in wash_labels.csv"

# -------------------------------------------------------------------
# IMPORTANT: Restrict evaluation to CLOTHING ONLY for this DEMO CSV
# -------------------------------------------------------------------
df_eval = df_all[df_all["is_clothing"] == 1].copy()
print("Rows used for demo batch inference (clothing only):", len(df_eval))

# Restrict to VAL split if you like:
# df_eval = df_eval[df_eval["split"] == "val"].copy()
# print("Rows in VAL split (clothing only):", len(df_eval))

# Ensure label columns are integer-typed
for col in ["color_label", "fabric_label", "wash_cycle_label", "is_clothing"]:
    assert col in df_eval.columns, f"Missing column: {col}"
    df_eval[col] = df_eval[col].astype(int)

num_color_classes   = df_eval["color_label"].nunique()
num_fabric_classes  = df_eval["fabric_label"].nunique()

NUM_WASH_HEAD       = 5              #
num_wash_classes    = NUM_WASH_HEAD
num_iscloth_classes = 2              # {0: NON_CLOTHING, 1: CLOTHING}

print(f"COLOR classes      : {num_color_classes}")
print(f"FABRIC classes     : {num_fabric_classes}")
print(f"WASH_CYCLE classes (in CSV subset): {df_eval['wash_cycle_label'].nunique()}")
print(f"WASH_CYCLE head size (model)     : {num_wash_classes}")
print(f"IS_CLOTHING classes: {num_iscloth_classes}")

# -------------------------------------------------------------------
# 1) Preprocessing transforms and demo dataset
# -------------------------------------------------------------------
IMG_SIZE = 256
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD  = [0.229, 0.224, 0.225]

eval_transform = transforms.Compose([
    transforms.Resize(IMG_SIZE + 32),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
])


class WashDemoDataset(Dataset):
    """
    Dataset for batch inference on df_eval (CLOTHING ONLY).

    Returns:
        X                 : image tensor
        image_path        : path string
        color_label_true  : int
        fabric_label_true : int
        wash_label_true   : int
        is_clothing_true  : int (always 1 here)
    """
    def __init__(self, df: pd.DataFrame, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = row["image_path"]

        img = Image.open(img_path).convert("RGB")
        if self.transform is not None:
            img = self.transform(img)

        return (
            img,
            img_path,
            int(row["color_label"]),
            int(row["fabric_label"]),
            int(row["wash_cycle_label"]),
            int(row["is_clothing"]),
        )


demo_dataset = WashDemoDataset(df_eval, transform=eval_transform)
demo_loader  = DataLoader(
    demo_dataset,
    batch_size=32,
    shuffle=False,
    num_workers=0,
    pin_memory=True,
)

# -------------------------------------------------------------------
# 2) 4-head model definition (must match training)
# -------------------------------------------------------------------
BACKBONE_NAME = "convnext_tiny"


class WashMultiTaskConvNeXt(nn.Module):
    """
    ConvNeXt backbone with four classification heads:
      - color
      - fabric
      - wash_cycle  (size must be 5 to match checkpoint)
      - is_clothing
    """
    def __init__(
        self,
        backbone_name: str = BACKBONE_NAME,
        num_color: int = 4,
        num_fabric: int = 5,
        num_wash: int = 5,
        num_iscloth: int = 2,
    ):
        super().__init__()
        self.backbone = timm.create_model(
            backbone_name,
            pretrained=False,    # we load fine-tuned weights from checkpoint
            num_classes=0,
            global_pool="avg",
        )
        feat_dim = self.backbone.num_features

        self.head_color      = nn.Linear(feat_dim, num_color)
        self.head_fabric     = nn.Linear(feat_dim, num_fabric)
        self.head_wash_cycle = nn.Linear(feat_dim, num_wash)
        self.head_is_cloth   = nn.Linear(feat_dim, num_iscloth)

    def forward(self, x):
        feat = self.backbone(x)
        logits_color      = self.head_color(feat)
        logits_fabric     = self.head_fabric(feat)
        logits_wash_cycle = self.head_wash_cycle(feat)
        logits_is_cloth   = self.head_is_cloth(feat)
        return logits_color, logits_fabric, logits_wash_cycle, logits_is_cloth


model = WashMultiTaskConvNeXt(
    backbone_name=BACKBONE_NAME,
    num_color=num_color_classes,
    num_fabric=num_fabric_classes,
    num_wash=num_wash_classes,        # = 5
    num_iscloth=num_iscloth_classes,
).to(device)

# Load checkpoint weights
state_dict = torch.load(CKPT_PATH, map_location=device)
model.load_state_dict(state_dict)
model.eval()

print("\nRunning batch inference on evaluation subset (CLOTHING ONLY) ...")

# -------------------------------------------------------------------
# 3) Inference loop and CSV export
# -------------------------------------------------------------------
rows_out = []

with torch.no_grad():
    for batch in tqdm(demo_loader, desc="Demo inference"):
        imgs, paths, y_c_true, y_f_true, y_w_true, y_is_true = batch

        imgs = imgs.to(device, non_blocking=True)

        logits_c, logits_f, logits_w, logits_is = model(imgs)

        y_c_pred  = logits_c.argmax(dim=1).cpu().numpy()
        y_f_pred  = logits_f.argmax(dim=1).cpu().numpy()
        y_w_pred  = logits_w.argmax(dim=1).cpu().numpy()
        y_is_pred = logits_is.argmax(dim=1).cpu().numpy()

        y_c_true  = y_c_true.numpy()
        y_f_true  = y_f_true.numpy()
        y_w_true  = y_w_true.numpy()
        y_is_true = y_is_true.numpy()

        for p, c_t, f_t, w_t, is_t, c_p, f_p, w_p, is_p in zip(
            paths, y_c_true, y_f_true, y_w_true, y_is_true,
            y_c_pred, y_f_pred, y_w_pred, y_is_pred
        ):
            rows_out.append({
                "image_path":         p,
                "color_label_true":   int(c_t),
                "fabric_label_true":  int(f_t),
                "wash_label_true":    int(w_t),
                "is_clothing_true":   int(is_t),   # should be 1 here
                "color_label_pred":   int(c_p),
                "fabric_label_pred":  int(f_p),
                "wash_label_pred":    int(w_p),
                "is_clothing_pred":   int(is_p),
            })

demo_df = pd.DataFrame(rows_out)
demo_csv_path = os.path.join(OUTPUT_DIR, "demo_predictions.csv")
demo_df.to_csv(demo_csv_path, index=False)

print("\nSaved demo predictions CSV to:", demo_csv_path)
print("Preview:")
print(demo_df.head())

"""---
### 19) Professional Single-Image Demo & Logging

In this step, I finalized the **production-ready single-image demo** for the multitask ConvNeXt model. The module:

- Loaded the **4-head checkpoint** (color, fabric, wash-cycle, is-clothing).
- Reconstructed all **index → label mappings** directly from the dataset and checkpoint to avoid mismatches.
- Implemented the high-level function  
  **`run_demo_on_image(img_path, show_image=True)`**, which:
  - Preprocessed the input image,
  - Ran a forward pass through all four heads,
  - Returned the predicted **color group**, **fabric group**, **washing program**, and **is-clothing state**,
  - Optionally displayed the annotated image,
  - Logged each call (timestamp + predictions) into  
    `outputs_multitask_wash/demo_usage_log.csv` for auditability.

This cell completed the **interactive professional demo pipeline** for the Smart Washing Assistant.

"""

# ============================================================
# Cell 20 — Professional Single-Image Demo + Logging (4-head)
# ------------------------------------------------------------
# High-level demo function:
#
#   run_demo_on_image(img_path: str, show_image: bool = True)
#
# 4-head multitask ConvNeXt model:
#   • COLOR        (garments only)
#   • FABRIC       (garments only)
#   • WASH_CYCLE   (garments only)
#   • IS_CLOTHING  (all images: 0 = NON_CLOTHING, 1 = CLOTHING)
# ============================================================

import os
import datetime
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torchvision import transforms
import timm

# -------------------------------------------------------------------
# 0) Paths, device, and metadata
# -------------------------------------------------------------------
WASH_ROOT   = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV  = os.path.join(WASH_ROOT, "wash_labels.csv")
OUTPUT_DIR  = os.path.join(WASH_ROOT, "outputs_multitask_wash")
CKPT_PATH   = os.path.join(OUTPUT_DIR, "best_model_wash.pt")

os.makedirs(OUTPUT_DIR, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

df_all = pd.read_csv(LABELS_CSV)
print("Loaded labels:", len(df_all))
print("Columns:", list(df_all.columns))

assert "is_clothing"      in df_all.columns, "'is_clothing' column missing."
assert "color_label"      in df_all.columns, "'color_label' column missing."
assert "fabric_label"     in df_all.columns, "'fabric_label' column missing."
assert "wash_cycle_label" in df_all.columns, "'wash_cycle_label' column missing."

# Restrict color/fabric mappings to CLOTHING rows
df_clothing = df_all[df_all["is_clothing"] == 1].copy()
print("Clothing rows used for mappings:", len(df_clothing))

num_color_classes  = df_clothing["color_label"].astype(int).nunique()
num_fabric_classes = df_clothing["fabric_label"].astype(int).nunique()

# --- IMPORTANT: take WASH + IS_CLOTHING output sizes from checkpoint ---
tmp_state = torch.load(CKPT_PATH, map_location="cpu")
num_wash_classes    = tmp_state["head_wash_cycle.weight"].shape[0]
num_iscloth_classes = tmp_state["head_is_cloth.weight"].shape[0]

print(f"COLOR classes      : {num_color_classes}")
print(f"FABRIC classes     : {num_fabric_classes}")
print(f"WASH_CYCLE classes : {num_wash_classes} (from checkpoint)")
print(f"IS_CLOTHING classes: {num_iscloth_classes} (from checkpoint)")

# Label index → human-readable name
color_map_df  = (
    df_clothing[["color_label",  "color_group"]]
    .dropna(subset=["color_label", "color_group"])
    .astype({"color_label": int})
    .drop_duplicates()
    .sort_values("color_label")
)
fabric_map_df = (
    df_clothing[["fabric_label", "fabric_group"]]
    .dropna(subset=["fabric_label", "fabric_group"])
    .astype({"fabric_label": int})
    .drop_duplicates()
    .sort_values("fabric_label")
)

# For wash_cycle, use all rows so all label indices (0..4) are covered
wash_map_df   = (
    df_all[["wash_cycle_label", "wash_cycle"]]
    .dropna(subset=["wash_cycle_label", "wash_cycle"])
    .astype({"wash_cycle_label": int})
    .drop_duplicates()
    .sort_values("wash_cycle_label")
)

idx2color  = dict(zip(color_map_df["color_label"],      color_map_df["color_group"]))
idx2fabric = dict(zip(fabric_map_df["fabric_label"],    fabric_map_df["fabric_group"]))
idx2wash   = dict(zip(wash_map_df["wash_cycle_label"],  wash_map_df["wash_cycle"]))

idx2iscloth = {0: "NON_CLOTHING", 1: "CLOTHING"}

# -------------------------------------------------------------------
# 1) Pre-processing pipeline for demo images
# -------------------------------------------------------------------
IMG_SIZE = 256
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD  = [0.229, 0.224, 0.225]

demo_transform = transforms.Compose([
    transforms.Resize(IMG_SIZE + 32),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
])

# -------------------------------------------------------------------
# 2) 4-head model definition (must match training/evaluation)
# -------------------------------------------------------------------
BACKBONE_NAME = "convnext_tiny"


class WashMultiTaskConvNeXt(nn.Module):
    """
    ConvNeXt backbone with four classification heads:
      • COLOR       (num_color_classes)
      • FABRIC      (num_fabric_classes)
      • WASH_CYCLE  (num_wash_classes)
      • IS_CLOTHING (num_iscloth_classes)
    """
    def __init__(self,
                 backbone_name: str = BACKBONE_NAME,
                 num_color: int = 4,
                 num_fabric: int = 5,
                 num_wash: int = 4,
                 num_iscloth: int = 2):
        super().__init__()
        self.backbone = timm.create_model(
            backbone_name,
            pretrained=False,   # weights are loaded from checkpoint
            num_classes=0,
            global_pool="avg",
        )
        feat_dim = self.backbone.num_features

        self.head_color      = nn.Linear(feat_dim, num_color)
        self.head_fabric     = nn.Linear(feat_dim, num_fabric)
        self.head_wash_cycle = nn.Linear(feat_dim, num_wash)
        self.head_is_cloth   = nn.Linear(feat_dim, num_iscloth)

    def forward(self, x):
        feat = self.backbone(x)
        logits_color      = self.head_color(feat)
        logits_fabric     = self.head_fabric(feat)
        logits_wash_cycle = self.head_wash_cycle(feat)
        logits_is_cloth   = self.head_is_cloth(feat)
        return logits_color, logits_fabric, logits_wash_cycle, logits_is_cloth


model = WashMultiTaskConvNeXt(
    backbone_name=BACKBONE_NAME,
    num_color=num_color_classes,
    num_fabric=num_fabric_classes,
    num_wash=num_wash_classes,
    num_iscloth=num_iscloth_classes,
).to(device)

state_dict = torch.load(CKPT_PATH, map_location=device)
model.load_state_dict(state_dict)
model.eval()

demo_log_path = os.path.join(OUTPUT_DIR, "demo_usage_log.csv")

# -------------------------------------------------------------------
# 3) Single-image DEMO function (4-head)
# -------------------------------------------------------------------
def run_demo_on_image(img_path: str, show_image: bool = True):
    """
    Run the trained 4-head model on a single image.
    """
    assert os.path.exists(img_path), f"Image not found: {img_path}"

    # Load and preprocess image
    img_pil = Image.open(img_path).convert("RGB")
    x = demo_transform(img_pil).unsqueeze(0).to(device)

    # Forward pass
    with torch.no_grad():
        logits_c, logits_f, logits_w, logits_is = model(x)
        pc  = logits_c.argmax(dim=1).item()
        pf  = logits_f.argmax(dim=1).item()
        pw  = logits_w.argmax(dim=1).item()
        pis = logits_is.argmax(dim=1).item()

    pred_color_name   = idx2color.get(pc,   f"color_{pc}")
    pred_fabric_name  = idx2fabric.get(pf, f"fabric_{pf}")
    pred_wash_name    = idx2wash.get(pw,   f"wash_{pw}")
    pred_iscloth_name = idx2iscloth.get(pis, f"iscloth_{pis}")

    # Console summary
    print("\n========================================")
    print(" SMART WASHING ASSISTANT — DEMO RESULT ")
    print("========================================")
    print(f"Image path           : {img_path}")
    print("----------------------------------------")
    print(f"Predicted IS_CLOTHING: {pred_iscloth_name}  (label = {pis})")
    print(f"Predicted COLOR      : {pred_color_name}    (label = {pc})")
    print(f"Predicted FABRIC     : {pred_fabric_name}   (label = {pf})")
    print(f"Predicted PROGRAM    : {pred_wash_name}     (label = {pw})")
    print("========================================\n")

    # visualization
    if show_image:
        plt.figure(figsize=(4, 4))
        plt.imshow(img_pil)
        plt.axis("off")
        plt.title(
            f"IS_CLOTHING: {pred_iscloth_name}\n"
            f"COLOR: {pred_color_name}\n"
            f"FABRIC: {pred_fabric_name}\n"
            f"WASH: {pred_wash_name}",
            fontsize=10,
        )
        plt.tight_layout()
        plt.show()

    # Append to usage log
    ts = datetime.datetime.now().isoformat(timespec="seconds")
    row = {
        "timestamp":             ts,
        "image_path":            img_path,
        "pred_is_clothing_label": pis,
        "pred_is_clothing_name":  pred_iscloth_name,
        "pred_color_label":       pc,
        "pred_color_group":       pred_color_name,
        "pred_fabric_label":      pf,
        "pred_fabric_group":      pred_fabric_name,
        "pred_wash_label":        pw,
        "pred_wash_cycle":        pred_wash_name,
    }

    if os.path.exists(demo_log_path):
        log_df = pd.read_csv(demo_log_path)
        log_df = pd.concat([log_df, pd.DataFrame([row])], ignore_index=True)
    else:
        log_df = pd.DataFrame([row])

    log_df.to_csv(demo_log_path, index=False)
    print("Logged DEMO call to:", demo_log_path)

    return {
        "color_group":       pred_color_name,
        "fabric_group":      pred_fabric_name,
        "wash_cycle":        pred_wash_name,
        "is_clothing":       pred_iscloth_name,
        "is_clothing_label": pis,
    }

# Example usage (uncomment if you want to test quickly):
# result = run_demo_on_image("/content/drive/MyDrive/wash_ai_project/demo/test_shirt.jpg")
# print(result)

!pip install gradio -q

"""---
### 20) Drive Mount & File Verification

In this step, I mounted Google Drive and verified that all required assets for the 4-head multitask system were correctly located.  
Specifically, I checked:

- **wash_labels.csv**  
- **best_model_wash.pt** (final 4-head checkpoint)  
- **ai.jpg** (header image used in the demo UI)

The cell confirmed that all essential files were present and ready for the demo and inference pipeline.

"""

# ============================================================
# Cell 21 — Mount Google Drive + Verify Project Paths
# ------------------------------------------------------------
# This cell:
#   • Mounts Google Drive
#   • Verifies locations of key project files for the 4-head model:
#       - wash_labels.csv
#       - best_model_wash.pt
#       - ai.jpg (header image for demo UI)
# ============================================================

from google.colab import drive
drive.mount('/content/drive')

import os

WASH_ROOT   = "/content/drive/MyDrive/wash_ai_project"
LABELS_CSV  = os.path.join(WASH_ROOT, "wash_labels.csv")
CKPT_PATH   = os.path.join(WASH_ROOT, "outputs_multitask_wash", "best_model_wash.pt")
HEADER_IMG  = os.path.join(WASH_ROOT, "ai.jpg")

print("Checking required files for the 4-head multitask project...\n")

print(f"wash_labels.csv          : {'FOUND' if os.path.exists(LABELS_CSV) else 'MISSING'}")
print(f"best_model_wash.pt (4-head model) : {'FOUND' if os.path.exists(CKPT_PATH) else 'MISSING'}")
print(f"ai.jpg (header for UI)   : {'FOUND' if os.path.exists(HEADER_IMG) else 'MISSING'}")

print("\nVerification complete.")

"""---
### 21) Dependency Installation & Ngrok Setup

In this stage, I installed all required packages for the interactive demo:

- **streamlit** for building the UI,  
- **pyngrok** for creating a secure public URL,  
- **timm** for loading the ConvNeXt backbone.

After installation, I configured Ngrok using the provided authentication token.  
The token was validated and successfully applied, confirming that the system was ready to launch the Streamlit demo externally.

"""

# ============================================================
# Cell 22 — Install Dependencies + Configure Ngrok
# ------------------------------------------------------------
# This cell installs:
#   • streamlit   — to run the interactive demo UI
#   • pyngrok     — to expose Streamlit via a public URL
#   • timm        — required for ConvNeXt (4-head multitask model)
# ============================================================

!pip install -q streamlit pyngrok timm

from pyngrok import ngrok

# ------------------------------------------------------------
# Ngrok authentication token (your real token)
# ------------------------------------------------------------
NGROK_TOKEN = "35ZUgpwfBDJaevljRpaSkWhvUTY_79Mg44P4M3rw7Ubpdvr9H"

# Simple safety check (will pass with this token)
if not NGROK_TOKEN or NGROK_TOKEN.strip() == "":
    raise ValueError(
        "\n✗ ERROR: Ngrok token is empty.\n"
        "Please paste your REAL authtoken into NGROK_TOKEN.\n"
        "Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\n"
    )

# Apply token
ngrok.set_auth_token(NGROK_TOKEN)
print("✓ Ngrok authtoken configured successfully for Streamlit demo.")

"""---
### 22) Streamlit Demo App (app.py) Summary

In this final stage, I created the complete **interactive Streamlit application** that demonstrates my 4-head multitask ConvNeXt model.  
The app loaded my trained checkpoint, handled single-image inputs, and produced three garment-level predictions:

- **Color group** (LIGHT / DARK / COLOR / COLORFUL)  
- **Fabric group** (COTTON / LINEN / WOOL / SILK / SYNTHETIC)  
- **Wash program** (human-readable program description)

The app automatically checked whether the uploaded image contained clothing using the **IS_CLOTHING** head.  
If the confidence was too low, the app safely rejected the image instead of producing wrong labels.

I also implemented:

- A clean preprocessing pipeline for consistent inference  
- Confidence-aware wash-cycle text generation  
- Automatic usage logging (timestamp + predictions)  
- Header image display  
- A responsive UI showing the uploaded image and the model’s recommendation side-by-side  

This **app.py** served as the final polished interface of my Smart Laundry Assistant, allowing anyone to upload a garment image and instantly get reliable washing guidance powered by deep learning.

"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# """
# AI Laundry Sorter – Streamlit Demo (4-Head Model Compatible)
# 
# This app:
#   • Loads the trained multitask ConvNeXt model (COLOR + FABRIC + WASH_CYCLE + IS_CLOTHING)
#   • Accepts a single clothing image as input
#   • Predicts:
#         - Color group   (LIGHT / DARK / COLOR / COLORFUL)
#         - Fabric group  (COTTON / LINEN / WOOL / SILK / SYNTHETIC)
#         - Washing program (human-readable text, no numeric labels in UI)
#   • Uses the IS_CLOTHING head + confidence heuristics to reject clearly non-garment images
# """
# 
# import os
# import io
# import datetime
# 
# import pandas as pd
# from PIL import Image
# 
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from torchvision import transforms
# import timm
# 
# import streamlit as st
# 
# # ============================================================
# # 0) Core paths
# # ============================================================
# WASH_ROOT   = "/content/drive/MyDrive/wash_ai_project"
# LABELS_CSV  = os.path.join(WASH_ROOT, "wash_labels.csv")
# CKPT_PATH   = os.path.join(WASH_ROOT, "outputs_multitask_wash", "best_model_wash.pt")
# HEADER_IMG  = os.path.join(WASH_ROOT, "ai.jpg")
# DEMO_LOG    = os.path.join(WASH_ROOT, "outputs_multitask_wash", "demo_usage_log.csv")
# 
# assert os.path.exists(LABELS_CSV), f"Labels CSV not found: {LABELS_CSV}"
# assert os.path.exists(CKPT_PATH),  f"Checkpoint not found: {CKPT_PATH}"
# os.makedirs(os.path.dirname(DEMO_LOG), exist_ok=True)
# 
# # ============================================================
# # 1) Label metadata (for mapping indices → names)
# # ============================================================
# df_all = pd.read_csv(LABELS_CSV)
# assert "is_clothing" in df_all.columns, "Column 'is_clothing' is missing in wash_labels.csv"
# 
# # ---------------- Clothing subset for COLOR / FABRIC heads -------------
# df_clothing = df_all[df_all["is_clothing"] == 1].copy()
# 
# df_clothing["color_label"]  = df_clothing["color_label"].astype(int)
# df_clothing["fabric_label"] = df_clothing["fabric_label"].astype(int)
# 
# num_color_classes  = df_clothing["color_label"].nunique()
# num_fabric_classes = df_clothing["fabric_label"].nunique()
# 
# # ---------------- WASH_CYCLE head: use ALL rows (should be 5 classes) ---
# wash_all = df_all[["wash_cycle_label", "wash_cycle"]].dropna(
#     subset=["wash_cycle_label", "wash_cycle"]
# ).copy()
# wash_all["wash_cycle_label"] = wash_all["wash_cycle_label"].astype(int)
# 
# num_wash_classes    = wash_all["wash_cycle_label"].nunique()  # should be 5
# num_iscloth_classes = 2  # {0: NON_CLOTHING, 1: CLOTHING}
# 
# print("num_color_classes :", num_color_classes)
# print("num_fabric_classes:", num_fabric_classes)
# print("num_wash_classes  :", num_wash_classes)
# 
# # Clean mappings: label → name
# color_map_df  = df_clothing[["color_label",  "color_group"]].drop_duplicates()
# fabric_map_df = df_clothing[["fabric_label", "fabric_group"]].drop_duplicates()
# wash_map_df   = wash_all[["wash_cycle_label", "wash_cycle"]].drop_duplicates()
# 
# color_map  = dict(zip(color_map_df["color_label"],  color_map_df["color_group"]))
# fabric_map = dict(zip(fabric_map_df["fabric_label"], fabric_map_df["fabric_group"]))
# wash_map   = dict(zip(wash_map_df["wash_cycle_label"], wash_map_df["wash_cycle"]))
# 
# #  Richer verbal descriptions for washing programs
# # Make sure keys here match your actual 'wash_cycle' strings.
# wash_full_description = {
#     "wool/delicate":       "Wool / Delicate – ~20°C, ultra-gentle agitation, very low spin.",
#     "delicate/hand-wash":  "Delicate / Hand-wash – 20–30°C, gentle cycle, low spin, ideal for silk and fine fabrics.",
#     "normal":              "Normal – 30–40°C, standard agitation and spin for everyday cotton/synthetics.",
#     "normal/delicate":     "Normal / Delicate – 30°C, slightly gentler mechanical action, medium spin.",
#     "synthetic/easy-care": "Synthetic / Easy-care – 30°C, anti-crease profile, moderate spin.",
#     # If you have additional wash_cycle names in the CSV, add/adjust them here.
# }
# 
# # ============================================================
# # 2) Model + preprocessing transforms
# # ============================================================
# IMG_SIZE = 256
# demo_transform = transforms.Compose([
#     transforms.Resize(IMG_SIZE + 32),
#     transforms.CenterCrop(IMG_SIZE),
#     transforms.ToTensor(),
#     transforms.Normalize(
#         [0.485, 0.456, 0.406],
#         [0.229, 0.224, 0.225],
#     ),
# ])
# 
# BACKBONE_NAME = "convnext_tiny"
# 
# 
# class WashMultiTaskConvNeXt(nn.Module):
#     """
#     Multitask ConvNeXt backbone (4 heads):
#       • head_color      → color_group classification
#       • head_fabric     → fabric_group classification
#       • head_wash_cycle → wash_cycle classification
#       • head_is_cloth   → is_clothing (0 = NON_CLOTHING, 1 = CLOTHING)
# 
#     Must match the architecture used in training.
#     """
#     def __init__(self, num_color, num_fabric, num_wash, num_iscloth=2):
#         super().__init__()
#         self.backbone = timm.create_model(
#             BACKBONE_NAME,
#             pretrained=False,
#             num_classes=0,
#             global_pool="avg",
#         )
#         feat_dim = self.backbone.num_features
# 
#         self.head_color      = nn.Linear(feat_dim, num_color)
#         self.head_fabric     = nn.Linear(feat_dim, num_fabric)
#         self.head_wash_cycle = nn.Linear(feat_dim, num_wash)
#         self.head_is_cloth   = nn.Linear(feat_dim, num_iscloth)
# 
#     def forward(self, x):
#         feat = self.backbone(x)
#         logits_color      = self.head_color(feat)
#         logits_fabric     = self.head_fabric(feat)
#         logits_wash_cycle = self.head_wash_cycle(feat)
#         logits_is_cloth   = self.head_is_cloth(feat)
#         return logits_color, logits_fabric, logits_wash_cycle, logits_is_cloth
# 
# 
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = WashMultiTaskConvNeXt(
#     num_color=num_color_classes,
#     num_fabric=num_fabric_classes,
#     num_wash=num_wash_classes,      # now consistent with checkpoint (5)
#     num_iscloth=num_iscloth_classes,
# ).to(device)
# 
# state_dict = torch.load(CKPT_PATH, map_location=device)
# model.load_state_dict(state_dict)
# model.eval()
# 
# # ============================================================
# # 3) Single-image prediction helper
# # ============================================================
# def predict_single_image(pil_img: Image.Image):
#     """
#     Run the trained multitask model on a single PIL image.
# 
#     Returns
#     -------
#     dict:
#       {
#         "color":  <predicted color_group or message>,
#         "fabric": <predicted fabric_group or message>,
#         "wash":   <human-readable wash program description>,
#       }
#     """
#     x = demo_transform(pil_img).unsqueeze(0).to(device)
# 
#     with torch.no_grad():
#         logits_c, logits_f, logits_w, logits_is = model(x)
# 
#         probs_c  = F.softmax(logits_c, dim=1)
#         probs_f  = F.softmax(logits_f, dim=1)
#         probs_w  = F.softmax(logits_w, dim=1)
#         probs_is = F.softmax(logits_is, dim=1)  # shape: [1, 2]
# 
#         max_pc, idx_c = probs_c.max(dim=1)
#         max_pf, idx_f = probs_f.max(dim=1)
#         max_pw, idx_w = probs_w.max(dim=1)
# 
#         max_pc = max_pc.item()
#         max_pf = max_pf.item()
#         max_pw = max_pw.item()
# 
#         # Probability that the image contains clothing (class 1)
#         p_is_cloth = probs_is[0, 1].item()
# 
#         VERY_LOW_CONF = 0.30
#         LOW_CONF      = 0.55
# 
#         # Non-garment gate: if clothing + color + fabric are all low-confidence
#         if (p_is_cloth < 0.5) and (max_pc < VERY_LOW_CONF) and (max_pf < VERY_LOW_CONF):
#             return {
#                 "color":  "No garment detected",
#                 "fabric": "No garment detected",
#                 "wash":   "No washing program suggested — the image does not appear to contain clothing.",
#             }
# 
#         low_conf_flag = (min(max_pc, max_pf, max_pw) < LOW_CONF) or (p_is_cloth < 0.6)
# 
#     # Decode labels
#     pc = idx_c.item()
#     pf = idx_f.item()
#     pw = idx_w.item()
# 
#     color_name  = color_map.get(pc,  f"Unknown (id={pc})")
#     fabric_name = fabric_map.get(pf, f"Unknown (id={pf})")
# 
#     wash_key = wash_map.get(pw, f"wash_{pw}")
#     full_wash_text = wash_full_description.get(wash_key, wash_key)
# 
#     if low_conf_flag:
#         full_wash_text = "[Low confidence] " + full_wash_text
# 
#     return {
#         "color":  color_name,
#         "fabric": fabric_name,
#         "wash":   full_wash_text,
#     }
# 
# # ============================================================
# # 4) Streamlit application
# # ============================================================
# def main():
#     st.set_page_config(
#         page_title="AI Laundry Sorter",
#         page_icon="🧺",
#         layout="centered",
#     )
# 
#     # header image
#     if os.path.exists(HEADER_IMG):
#         st.image(HEADER_IMG, use_container_width=True)
# 
#     st.title("AI Laundry Sorter")
#     st.caption(
#         "Multitask ConvNeXt (4-head) for automatic color, fabric, "
#         "and washing program recommendations."
#     )
# 
#     uploaded_file = st.file_uploader(
#         "Upload a clothing image (JPG or PNG)",
#         type=["jpg", "jpeg", "png"],
#     )
# 
#     if uploaded_file is not None:
#         pil_img = Image.open(io.BytesIO(uploaded_file.read())).convert("RGB")
# 
#         col1, col2 = st.columns(2)
# 
#         with col1:
#             st.subheader("Input Image")
#             st.image(pil_img, use_container_width=True)
# 
#         result = predict_single_image(pil_img)
# 
#         with col2:
#             st.subheader("AI Recommendation")
#             st.markdown(f"**Color Group:** {result['color']}")
#             st.markdown(f"**Fabric Group:** {result['fabric']}")
#             st.markdown(f"**Wash Program:** {result['wash']}")
# 
#         # Log usage
#         ts = datetime.datetime.now().isoformat(timespec="seconds")
#         log_row = pd.DataFrame([{
#             "timestamp":   ts,
#             "image_name":  uploaded_file.name,
#             "pred_color":  result["color"],
#             "pred_fabric": result["fabric"],
#             "pred_wash":   result["wash"],
#         }])
# 
#         if os.path.exists(DEMO_LOG):
#             old = pd.read_csv(DEMO_LOG)
#             pd.concat([old, log_row], ignore_index=True).to_csv(DEMO_LOG, index=False)
#         else:
#             log_row.to_csv(DEMO_LOG, index=False)
# 
#         st.success("Prediction logged.")
#     else:
#         st.info("Upload a garment image to receive an automatic washing recommendation.")
# 
# if __name__ == "__main__":
#     main()
#

"""---
### 23) Deployment Package Summary

In this step, I packaged the entire 4-head multitask washing-assistant system into a **single deployable ZIP file**.  
The archive was built with a flat structure so that any hosting environment (Streamlit Cloud, local machine, or server/Docker setup) can run it immediately.

The ZIP included:

- **app.py** — the full Streamlit interface  
- **requirements.txt** — all Python dependencies  
- **wash_labels.csv** — final cleaned label table  
- **best_model_wash.pt** — trained 4-head ConvNeXt checkpoint  
- **ai.jpg** — header image for the UI  

After assembling the archive using `zip -j`, the notebook automatically triggered a **download** so I could store the deployment package locally.

"""

# ============================================================
# Cell 25 — Build & Download Deployment Package
# ------------------------------------------------------------
# This cell generates a ready-to-deploy ZIP file containing:
#       • app.py
#       • requirements.txt
#       • wash_labels.csv
#       • best_model_wash.pt
#       • ai.jpg (header image)
#
# The ZIP file can be used for:
#       • Streamlit Cloud
#       • Local deployment
#       • Server / Docker setups
# ============================================================

import os
from google.colab import files

# ------------------------------------------------------------
# Project root
# ------------------------------------------------------------
WASH_ROOT = "/content/drive/MyDrive/wash_ai_project"

# ------------------------------------------------------------
# Required input paths
# ------------------------------------------------------------
APP_PATH   = "app.py"   # app.py must already exist in the current directory
REQ_PATH   = "/content/requirements.txt"

LABELS_CSV = os.path.join(WASH_ROOT, "wash_labels.csv")
MODEL_PT   = os.path.join(WASH_ROOT, "outputs_multitask_wash", "best_model_wash.pt")
HEADER_IMG = os.path.join(WASH_ROOT, "ai.jpg")

# ------------------------------------------------------------
# Output ZIP filename
# ------------------------------------------------------------
ZIP_NAME = "ai_laundry_sorter_deploy.zip"

# ------------------------------------------------------------
# Create the ZIP file (flat structure using -j)
# ------------------------------------------------------------
!zip -j "$ZIP_NAME" \
    "$APP_PATH" \
    "$REQ_PATH" \
    "$LABELS_CSV" \
    "$MODEL_PT" \
    "$HEADER_IMG"

# ------------------------------------------------------------
# Download ZIP to local machine
# ------------------------------------------------------------
files.download(ZIP_NAME)

"""---
### Conclusion

In this project, **I designed and implemented a complete end-to-end AI Laundry Sorter**, built on a 4-head ConvNeXt multitask architecture. I personally handled every stage — from dataset preparation and label construction to model training, evaluation, inference logic, visualization, and deployment.

The model predicts **color group**, **fabric group**, **washing program**, and **is_clothing**, all in one forward pass. Using one shared backbone with four specialized heads proved highly suitable because garments contain intertwined visual cues: color, fabric, and washing requirements are not independent. A multitask model is therefore more stable, more data-efficient, and significantly faster than maintaining separate networks.

I created a **single-image demo**, **batch inference pipeline**, **confidence-aware rejection logic**, **usage logging**, and a **professional Streamlit interface**. I also produced a clean deployment package containing `app.py`, the trained checkpoint, metadata, and all required dependencies — ready for local, cloud, or server deployment.

You can view my deployed demo here:  
🔗 **https://c3bnqhq9vkb86zcppbcggh.streamlit.app/**

#### Where this system can be used
I can deploy this solution in:
- **Smart washing machines** (automatic program recommendation from a single photo)  
- **Mobile laundry assistant apps** that advise users about delicate garments  
- **Retail platforms** for automated garment tagging and wash-instruction prediction  
- **Warehouse or inventory systems** requiring rapid multi-attribute garment classification  
- **Assistive technologies** for users who need simplified washing guidance  

#### Why this model is well-suited
- The **multitask ConvNeXt architecture** learns richer shared features across fabric, color, and wash-cycle.  
- The **IS_CLOTHING head** protects the system from giving incorrect predictions on non-garment images.  
- It performs well on **natural, real-life photos**, not just studio images.  
- The entire workflow is **reproducible, packaged, and deployment-ready**.  
- ConvNeXt offers strong performance on fine textures and patterns — crucial for fabric understanding.

#### What I learned
Through this project, I strengthened my ability to:
- Build a **complex multitask deep learning framework** from scratch  
- Design and manage **coherent multi-head labeling systems**  
- Implement **robust inference logic**, confidence thresholds, and safety gates  
- Develop **production-level UI** using Streamlit + Ngrok  
- Organize and export a complete deployment package for **real-world use**  
- Integrate data engineering, model design, evaluation, and UI development in one cohesive system  

Overall, I engineered a robust, intelligent, and fully deployable **AI Laundry Sorting Assistant** — capable of supporting real-world applications across consumer devices, retail systems, and smart appliances.

---
### Resources

1. **H&M Fashion Image Dataset (Kaggle)**  
   https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations  
   Used as the primary dataset for clothing images, enabling supervised training for color, fabric, and wash-cycle classification.

2. **PyTorch Deep Learning Framework**  
   https://pytorch.org/  
   Core framework for building, training, and evaluating the multi-task ConvNeXt model.

3. **timm — PyTorch Image Models**  
   https://github.com/huggingface/pytorch-image-models  
   Library providing state-of-the-art backbones, including ConvNeXt, used in this project.

4. **Streamlit Web Framework**  
   https://streamlit.io/  
   Used to develop the interactive AI Laundry Sorter demo interface.

5. **Ngrok Secure Tunnel Service**  
   https://ngrok.com/  
   Enables secure public URLs for deploying the Streamlit demo from Google Colab.
"""

